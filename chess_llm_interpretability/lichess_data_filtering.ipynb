{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data begins as a bunch of PGN transcripts. However, to work in tensors we need all transcripts to be the same length. So, this file takes our PGNs and performs some filtering.\n",
    "\n",
    "This notebook has a very similar counterpart, `utils\\chess_gpt_eval_data_filtering.ipynb`. The lichess and chess_gpt_eval datasets have a different structure and different column names. For most peoples' needs, the lichess dataset alone should suffice, so I made two separate notebooks to keep this one simple.\n",
    "\n",
    "The output of this file is 4 different csv's:\n",
    "\n",
    "`lichess_100mb.csv`\" 100 MB of lichess PGN games, with every game also containing player Elo information.\n",
    "\n",
    "`lichess_100mb_filtered.csv`: We perform some filtering for game length, add player Elo bucket, and do some manipulation of the PGN string.\n",
    "\n",
    "`lichess_train.csv` and `lichess_test.csv` a 50 / 50 train / test split of `lichess_100mb_filtered.csv`, used for training and testing linear probes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "prefix = \"lichess_\"\n",
    "\n",
    "\n",
    "input_file = f'{DATA_DIR}{prefix}100mb.csv'\n",
    "output_file = input_file.replace(\".csv\", \"_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we download the dataset if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_file):\n",
    "    dataset_path = \"adamkarvonen/chess_games\"\n",
    "    file_path = f\"{prefix}100mb.zip\"\n",
    "    # No idea why streaming=True is required to avoid an error here. Huggingface ¯\\_(ツ)_/¯\n",
    "    dataset = load_dataset(dataset_path, data_files=file_path,streaming=True)\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our LLMs need a delimiter token \";\" at the beginning of every PGN string or it won't work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_file)\n",
    "\n",
    "def format_transcript(game: str) -> str:\n",
    "    new_game = ';' + game\n",
    "    return new_game\n",
    "\n",
    "df['transcript'] = df['transcript'].apply(format_transcript)\n",
    "\n",
    "for game in df.head()['transcript']:\n",
    "    print(game)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter all games to be len 365. This means we discard anything under that length. I chose 365 because that's the 50% of df.describe(). I also count the number of moves (with x.split()) and discard anything below the 25th percentile. This makes it easier if I want to do any move based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_df = df['transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "game_length_in_chars = 365\n",
    "\n",
    "# Data setup. All games must have same length. 50% are >= 690 moves. I will discard all games less than 680, and truncate the rest to 680.\n",
    "filtered_df = df[df['transcript'].apply(lambda x: len(x) >= game_length_in_chars)].copy()\n",
    "filtered_df.loc[:, 'transcript'] = filtered_df['transcript'].apply(lambda x: x[:game_length_in_chars])\n",
    "\n",
    "len_df = filtered_df['transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
    "move_count = move_count_df.describe()\n",
    "print(\"move count\", move_count_df.describe())\n",
    "quarter_percentile = move_count['25%']\n",
    "print(\"quarter percentile\", quarter_percentile)\n",
    "\n",
    "# Now I need to filter out games that are too short. I will discard all games less than 25th percentile  moves.\n",
    "filtered_df = filtered_df[filtered_df['transcript'].apply(lambda x: len(x.split()) >= quarter_percentile)]\n",
    "print(filtered_df.describe())\n",
    "print(filtered_df.head())\n",
    "\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "move_count_df = filtered_df['transcript'].apply(lambda x: len(x.split()))\n",
    "print(move_count_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_df))\n",
    "print(filtered_df['WhiteElo'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification task, I wanted some Elo bins for the probe to classify. This somewhat arbitrarily creates 6 different Elo bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create binned columns and bin index columns\n",
    "def create_binned_columns(df, column_name):\n",
    "\n",
    "    # Ensure column is numeric and handle NaN values. Here, we choose to drop them, but you might fill them instead.\n",
    "    if df[column_name].dtype.kind not in 'biufc' or pd.isnull(df[column_name]).any():\n",
    "        df = df.dropna(subset=[column_name])\n",
    "        df[column_name] = pd.to_numeric(df[column_name], errors='coerce')\n",
    "\n",
    "    binned_column_name = f'{column_name}Binned'\n",
    "    bin_index_column_name = f'{column_name}BinIndex'\n",
    "    \n",
    "    # Create quantile-based bins\n",
    "    num_bins = 6\n",
    "    # Create quantile-based bins with range labels, dropping duplicates if necessary\n",
    "    df[binned_column_name], bins = pd.qcut(df[column_name], q=num_bins, retbins=True, duplicates='drop')\n",
    "\n",
    "    # Convert bin labels to strings and assign to the column\n",
    "    df[binned_column_name] = df[binned_column_name].apply(lambda x: f'({x.left}, {x.right}]')\n",
    "\n",
    "    # Create bin index column\n",
    "    df[bin_index_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Apply the function to both WhiteElo and BlackElo\n",
    "create_binned_columns(filtered_df, 'WhiteElo')\n",
    "create_binned_columns(filtered_df, 'BlackElo')\n",
    "\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "\n",
    "# Histogram for WhiteElo\n",
    "axes[0].hist(filtered_df['WhiteElo'], bins=30, color='blue', alpha=0.7)\n",
    "axes[0].set_title('WhiteElo Distribution')\n",
    "axes[0].set_xlabel('WhiteElo')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Bar chart for WhiteEloBinned\n",
    "bin_counts = filtered_df['WhiteEloBinned'].value_counts()\n",
    "axes[1].bar(bin_counts.index.astype(str), bin_counts.values, color='green', alpha=0.7)\n",
    "axes[1].set_title('WhiteElo Binned Distribution')\n",
    "axes[1].set_xlabel('WhiteElo Bins')\n",
    "axes[1].set_ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['WhiteEloBinned'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle all rows of the dataset\n",
    "\n",
    "df = pd.read_csv(output_file)\n",
    "df = df.sample(frac=1, random_state=200).reset_index(drop=True)\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_file)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Split df into a train and test split\n",
    "train = df.sample(frac=0.5, random_state=200)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# Save the train and test splits to csv\n",
    "train.to_csv(f'{DATA_DIR}{prefix}train.csv', index=False)\n",
    "test.to_csv(f'{DATA_DIR}{prefix}test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "othello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
