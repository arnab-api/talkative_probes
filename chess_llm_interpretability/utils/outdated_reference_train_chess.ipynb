{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import einops\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from fancy_einsum import einsum\n",
    "import chess\n",
    "import numpy as np\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import chess_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags to control logging\n",
    "debug_mode = False\n",
    "info_mode = True\n",
    "\n",
    "if debug_mode:\n",
    "    log_level = logging.DEBUG\n",
    "elif info_mode:\n",
    "    log_level = logging.INFO\n",
    "else:\n",
    "    log_level = logging.WARNING\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=log_level)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"models/\"\n",
    "DATA_DIR = \"data/\"\n",
    "PROBE_DIR = \"linear_probes/\"\n",
    "DATASET_PREFIX = \"lichess_\"\n",
    "# DATASET_PREFIX = \"stockfish_\"\n",
    "SPLIT = \"train\"\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    min_val: int\n",
    "    max_val: int\n",
    "    custom_function: callable\n",
    "    linear_probe_name: str\n",
    "    num_rows: int = 8\n",
    "    num_cols: int = 8\n",
    "    df_file: str = f\"{DATA_DIR}{DATASET_PREFIX}{SPLIT}.csv\"\n",
    "    column_name: str = None\n",
    "    probing_for_skill: bool = False\n",
    "\n",
    "piece_config = Config(\n",
    "    min_val = -6,\n",
    "    max_val = 6,\n",
    "    custom_function = chess_utils.board_to_piece_state,\n",
    "    linear_probe_name = \"chess_piece_probe\",\n",
    ")\n",
    "\n",
    "color_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_piece_color_state,\n",
    "    linear_probe_name=\"chess_color_probe\",\n",
    ")\n",
    "\n",
    "random_config = Config(\n",
    "    min_val = -1,\n",
    "    max_val = 1,\n",
    "    custom_function=chess_utils.board_to_random_state,\n",
    "    linear_probe_name=\"chess_random_probe\",\n",
    ")\n",
    "\n",
    "# NOTE: skill_config should only be used with the stockfish_ dataset\n",
    "skill_config = Config(\n",
    "    min_val = -2,\n",
    "    max_val = 20,\n",
    "    custom_function=chess_utils.board_to_skill_state,\n",
    "    linear_probe_name=\"chess_skill_probe\",\n",
    "    num_rows = 1,\n",
    "    num_cols= 1,\n",
    "    column_name = \"player_two\",\n",
    "    probing_for_skill=True,\n",
    ")\n",
    "\n",
    "# NOTE: elo_binned_config should only be used with the lichess_ dataset\n",
    "elo_binned_config = Config(\n",
    "    min_val = 0,\n",
    "    max_val = 5, # NOTE: Extreme jank here. max_val should be set to num_bins in lichess_data_filtering.ipynb TODO: Fix this\n",
    "    custom_function=chess_utils.board_to_skill_state,\n",
    "    linear_probe_name=\"chess_skill_probe\",\n",
    "    num_rows = 1,\n",
    "    num_cols= 1,\n",
    "    column_name = \"WhiteEloBinIndex\",\n",
    "    probing_for_skill=True,\n",
    ")\n",
    "\n",
    "# NOTE: elo_config should only be used with the lichess_ dataset\n",
    "# NOTE: elo_config should only be used with MSE loss. min_val and max_val are meaningless here.\n",
    "elo_config = Config(\n",
    "    min_val = 0,\n",
    "    max_val = 5,\n",
    "    custom_function=chess_utils.board_to_skill_state,\n",
    "    linear_probe_name=\"chess_skill_probe\",\n",
    "    num_rows = 1,\n",
    "    num_cols= 1,\n",
    "    column_name = \"WhiteElo\",\n",
    "    probing_for_skill=True,\n",
    ")\n",
    "\n",
    "config = piece_config\n",
    "# config = color_config\n",
    "# config = random_config\n",
    "# config = skill_config\n",
    "# config = elo_binned_config\n",
    "config = elo_config\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "n_layers = 16\n",
    "n_heads = 8\n",
    "\n",
    "model_name = \"tf_lens_lichess_16_ckpt\"\n",
    "model_name = \"tf_lens_16\"\n",
    "\n",
    "PROCESS_DATA = False\n",
    "levels_of_interest = [0, 2] # NOTE: This is only used if PROCESS_DATA is True\n",
    "TRAIN_WITH_MSE = False\n",
    "NORMALIZE_SKILL_FOR_MSE = True\n",
    "wandb_logging = False\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "custom_indexing_function = chess_utils.find_dots_indices\n",
    "\n",
    "batch_size = 5\n",
    "num_epochs = 1\n",
    "pos_start = 5 # indexes into custom_indexing_function. Example: if pos_start = 25, for find_dots_indices, selects everything after the first 25 moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = n_layers,\n",
    "    d_model = 512,\n",
    "    d_head = 64,\n",
    "    n_heads = n_heads,\n",
    "    d_mlp = 2048,\n",
    "    d_vocab = 32,\n",
    "    n_ctx = 1023,\n",
    "    act_fn=\"gelu\",\n",
    "    normalization_type=\"LNPre\"\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "model.load_state_dict(torch.load(f'{MODEL_DIR}{model_name}.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta is used to encode the string pgn strings into integer sequences\n",
    "with open(\"models/meta.pkl\", \"rb\") as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "logger.info(meta)\n",
    "\n",
    "stoi, itos = meta[\"stoi\"], meta[\"itos\"]\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "logger.info(encode(\"1.e4 e6 2.Nf3 d5 3.Nc3 d4 4.Ne2 c5 5.c3 d3 6.Nf4 c4 7.Qa4+ Bd7 8.Qxc4 Nf6 9.e5 Ng4 10.h3 Nxf2 11.Kxf2 Qb6+ 12.Ke1 Bb5 13.Qc8+ Ke7 14.Bxd3 Bd7 15.Qc4 Nc6 16.Be4 Rc8 17.Qb3 Qc7 18.d4 Rb8 19.Be3 Na5 20.Qd1 g6 21.Bd3 Bg7 22.Rf1 Nc6 23.Kf2 Rhe8 24.Kg1 h6 25.Rc1 g5 26.Nh5 Bh8 27.Nd2 Qb6 28.Nf6 Red8 29.Nxd7 Rxd7 30.Qf3 Qxb2 31.Qxf7+ Kd8 32.Qf8+\"))\n",
    "logger.info(decode(encode(\";1.e4 \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_filename = config.df_file\n",
    "processing_df_filename = f\"{DATA_DIR}temporary_in_processing.csv\"\n",
    "\n",
    "df = pd.read_csv(input_df_filename)\n",
    "df.to_csv(processing_df_filename, index=False)\n",
    "user_state_dict_one_hot_mapping = None\n",
    "\n",
    "if PROCESS_DATA:\n",
    "    df = pd.read_csv(processing_df_filename)\n",
    "    user_state_dict_one_hot_mapping = {}\n",
    "    for i in range(len(levels_of_interest)):\n",
    "        user_state_dict_one_hot_mapping[levels_of_interest[i]] = i\n",
    "\n",
    "    matches = {number for number in levels_of_interest}\n",
    "    logger.info(f\"Levels to be used in probe dataset: {matches}\")\n",
    "    \n",
    "    # Filter the DataFrame based on these matches\n",
    "    filtered_df = df[df[config.column_name].isin(matches)]\n",
    "    filtered_df.to_csv(processing_df_filename, index=False)\n",
    "    logger.info(f\"Number of games in filtered dataset: {len(filtered_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processing_df_filename)\n",
    "\n",
    "df = pd.read_csv(f\"{processing_df_filename}\")\n",
    "row_length = len(df[\"transcript\"].iloc[0])\n",
    "num_games = len(df)\n",
    "\n",
    "assert all(\n",
    "    df[\"transcript\"].apply(lambda x: len(x) == row_length)\n",
    "), \"Not all transcripts are of length {}\".format(row_length)\n",
    "\n",
    "board_seqs_string = df[\"transcript\"]\n",
    "\n",
    "logger.info(f'Number of games: {len(board_seqs_string)},length of a game in chars: {len(board_seqs_string[0])}')\n",
    "\n",
    "assert (len(board_seqs_string), len(board_seqs_string[0])) == (num_games, row_length)\n",
    "\n",
    "encoded_df = df[\"transcript\"].apply(encode)\n",
    "logger.info(encoded_df.head())\n",
    "board_seqs_int = torch.tensor(encoded_df.apply(list).tolist())\n",
    "logger.info(f\"board_seqs_int shape: {board_seqs_int.shape}\")\n",
    "assert board_seqs_int.shape == (num_games, row_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_stack = None\n",
    "test_skill = None\n",
    "\n",
    "std = 1 # Default value if NORMALIZE_SKILL_FOR_MSE is False\n",
    "\n",
    "if config.probing_for_skill == True:\n",
    "    df = pd.read_csv(f\"{processing_df_filename}\")\n",
    "    skill_levels_list = df[config.column_name].tolist()\n",
    "\n",
    "    skill_stack = torch.tensor(skill_levels_list)\n",
    "    logger.info(f\"Unique values in skill_stack: {skill_stack.unique()}\")\n",
    "    logger.info(f\"skill_stack shape: {skill_stack.shape}\")\n",
    "    assert skill_stack.shape == (num_games,)\n",
    "\n",
    "    if TRAIN_WITH_MSE and NORMALIZE_SKILL_FOR_MSE:\n",
    "        skill_stack = skill_stack.to(dtype=torch.float32) # necessary for mean and std to be float32\n",
    "        mean = skill_stack.mean()\n",
    "        std = skill_stack.std()\n",
    "\n",
    "        # Normalize the target values\n",
    "        skill_stack = (skill_stack - mean) / std\n",
    "\n",
    "    test_skill = skill_stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing_function_name = custom_indexing_function.__name__\n",
    "\n",
    "custom_indices = chess_utils.find_custom_indices(processing_df_filename, custom_indexing_function)\n",
    "custom_indices = torch.tensor(custom_indices).long()\n",
    "# logger.debug(state_stack.shape)\n",
    "logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "_, shortest_length = custom_indices.shape\n",
    "\n",
    "assert custom_indices.shape == (num_games, shortest_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_stack = torch.tensor(chess_utils.create_state_stack(board_seqs_string[0], config.custom_function, test_skill)).long()\n",
    "logger.info(f\"A single state_stack shape: {state_stack.shape}\")\n",
    "\n",
    "state_stacks = chess_utils.create_state_stacks(board_seqs_string[:50], config.custom_function, skill_stack)\n",
    "logger.info(f\"state_stack shape: {state_stacks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = 0.01\n",
    "# pos_end = model.cfg.n_ctx - 5\n",
    "# input_length = 680\n",
    "# pos_end = input_length - 0\n",
    "# length = pos_end - pos_start\n",
    "one_hot_range = config.max_val - config.min_val + 1\n",
    "if PROCESS_DATA:\n",
    "    one_hot_range = len(levels_of_interest)\n",
    "# I use min because the probes seem to converge within 25k games\n",
    "num_games = min(((len(board_seqs_int) // batch_size) * batch_size), (25000 // batch_size) * batch_size) # Unfortunately, num_games must be divisible by batch_size TODO: Fix this\n",
    "modes = 1\n",
    "\n",
    "max_lr = 3e-4\n",
    "min_lr = max_lr / 10\n",
    "max_iters = num_games * num_epochs\n",
    "decay_lr = True\n",
    "\n",
    "state_stack_one_hot = chess_utils.state_stack_to_one_hot(modes, config.num_rows, config.num_cols, config.min_val, config.max_val, device, state_stacks, user_state_dict_one_hot_mapping)\n",
    "logger.info(f\"state_stack_one_hot shape: {state_stack_one_hot.shape}\\n\")\n",
    "logger.info(f\"Note: This will only be meaningful if training on board state: \\n{state_stack_one_hot[:, 1, 170, 4:9, 2:5]}\")\n",
    "logger.info(f\"Note: This will only be meaningful if training on board state: \\n{state_stacks[:, 1, 170, 4:9, 2:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(current_iter: int, max_iters: int, max_lr: float, min_lr: float) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the learning rate using linear decay.\n",
    "\n",
    "        Args:\n",
    "        - current_iter (int): The current iteration.\n",
    "        - max_iters (int): The total number of iterations for decay.\n",
    "        - lr (float): The initial learning rate.\n",
    "        - min_lr (float): The minimum learning rate after decay.\n",
    "\n",
    "        Returns:\n",
    "        - float: The calculated learning rate.\n",
    "        \"\"\"\n",
    "        # Ensure current_iter does not exceed max_iters\n",
    "        current_iter = min(current_iter, max_iters)\n",
    "\n",
    "        # Calculate the linearly decayed learning rate\n",
    "        decayed_lr = max_lr - (max_lr - min_lr) * (current_iter / max_iters)\n",
    "\n",
    "        return decayed_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_probe_cross_entropy(layer: int):\n",
    "    linear_probe_name = (\n",
    "        f\"{PROBE_DIR}{model_name}_{config.linear_probe_name}_layer_{layer}.pth\"\n",
    "    )\n",
    "    linear_probe = torch.randn(\n",
    "        modes,\n",
    "        model.cfg.d_model,\n",
    "        config.num_rows,\n",
    "        config.num_cols,\n",
    "        one_hot_range,\n",
    "        requires_grad=False,\n",
    "        device=device,\n",
    "    ) / np.sqrt(model.cfg.d_model)\n",
    "    linear_probe.requires_grad = True\n",
    "    logger.info(f\"linear_probe shape: {linear_probe.shape}\")\n",
    "    lr = max_lr\n",
    "    optimiser = torch.optim.AdamW(\n",
    "        [linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd\n",
    "    )\n",
    "\n",
    "    logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "    # logger.debug(dots_indices.shape)\n",
    "\n",
    "    if wandb_logging:\n",
    "        import wandb\n",
    "\n",
    "        wandb_project = \"chess_linear_probes\"\n",
    "        wandb_run_name = f\"{config.linear_probe_name}_{model_name}_layer_{layer}_indexing_{indexing_function_name}\"\n",
    "        if PROCESS_DATA:\n",
    "            wandb_run_name += \"_levels\"\n",
    "            for level in levels_of_interest:\n",
    "                wandb_run_name += f\"_{level}\"\n",
    "\n",
    "        logging_dict = {\n",
    "            \"linear_probe_name\": config.linear_probe_name,\n",
    "            \"model_name\": model_name,\n",
    "            \"layer\": layer,\n",
    "            \"indexing_function_name\": indexing_function_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"max_lr\": max_lr,\n",
    "            \"wd\": wd,\n",
    "            \"pos_start\": pos_start,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"num_games\": num_games,\n",
    "            \"modes\": modes,\n",
    "            \"one_hot_range\": one_hot_range,\n",
    "            \"wandb_project\": wandb_project,\n",
    "            \"wandb_run_name\": wandb_run_name,\n",
    "        }\n",
    "        wandb.init(project=wandb_project, name=wandb_run_name, config=logging_dict)\n",
    "\n",
    "    current_iter = 0\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        full_train_indices = torch.randperm(num_games)\n",
    "        for i in tqdm(range(0, num_games, batch_size)):\n",
    "            lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "            for param_group in optimiser.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "            indices = full_train_indices[i : i + batch_size]\n",
    "            list_of_indices = (\n",
    "                indices.tolist()\n",
    "            )  # For indexing into the board_seqs_string list of strings\n",
    "            # logger.debug(list_of_indices)\n",
    "            games_int = board_seqs_int[indices]\n",
    "            games_int = games_int[:, :]\n",
    "            # logger.debug(games_int.shape)\n",
    "            games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "            games_str = [s[:] for s in games_str]\n",
    "            games_dots = custom_indices[indices]\n",
    "            games_dots = games_dots[:, pos_start:]\n",
    "            # logger.debug(games_dots.shape)\n",
    "\n",
    "            if config.probing_for_skill:\n",
    "                games_skill = skill_stack[indices]\n",
    "                # logger.debug(games_skill.shape)\n",
    "            else:\n",
    "                games_skill = None\n",
    "\n",
    "            state_stack = chess_utils.create_state_stacks(\n",
    "                games_str, config.custom_function, games_skill\n",
    "            )\n",
    "            # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "            # logger.debug(state_stack.shape)\n",
    "            # Initialize a list to hold the indexed state stacks\n",
    "            indexed_state_stacks = []\n",
    "\n",
    "            for batch_idx in range(batch_size):\n",
    "                # Get the indices for the current batch\n",
    "                dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                # Index the state_stack for the current batch\n",
    "                indexed_state_stack = state_stack[\n",
    "                    :, batch_idx, dots_indices_for_batch, :, :\n",
    "                ]\n",
    "\n",
    "                # Append the result to the list\n",
    "                indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "            # Stack the indexed state stacks along the first dimension\n",
    "            # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "            state_stack = torch.stack(indexed_state_stacks)\n",
    "\n",
    "            # Use einops to rearrange the dimensions after stacking\n",
    "            state_stack = einops.rearrange(\n",
    "                state_stack, \"batch modes pos row col -> modes batch pos row col\"\n",
    "            )\n",
    "\n",
    "            # logger.debug(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "            state_stack_one_hot = chess_utils.state_stack_to_one_hot(\n",
    "                modes,\n",
    "                config.num_rows,\n",
    "                config.num_cols,\n",
    "                config.min_val,\n",
    "                config.max_val,\n",
    "                device,\n",
    "                state_stack,\n",
    "                user_state_dict_one_hot_mapping,\n",
    "            ).to(device)\n",
    "            \n",
    "            # logger.debug(state_stack_one_hot.shape)\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(\n",
    "                    games_int.to(device)[:, :-1], return_type=None\n",
    "                )\n",
    "                resid_post = cache[\"resid_post\", layer][:, :]\n",
    "            # Initialize a list to hold the indexed state stacks\n",
    "            indexed_resid_posts = []\n",
    "\n",
    "            for batch_idx in range(games_dots.size(0)):\n",
    "                # Get the indices for the current batch\n",
    "                dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                # Index the state_stack for the current batch\n",
    "                indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "                # Append the result to the list\n",
    "                indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "            # Stack the indexed state stacks along the first dimension\n",
    "            # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "            resid_post = torch.stack(indexed_resid_posts)\n",
    "            # logger.debug(\"Resid post\", resid_post.shape)\n",
    "            probe_out = einsum(\n",
    "                \"batch pos d_model, modes d_model rows cols options -> modes batch pos rows cols options\",\n",
    "                resid_post,\n",
    "                linear_probe,\n",
    "            )\n",
    "            # logger.debug(probe_out.shape, state_stack_one_hot.shape, state_stack.shape)\n",
    "\n",
    "            assert probe_out.shape == state_stack_one_hot.shape\n",
    "\n",
    "            accuracy = (\n",
    "                (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1))\n",
    "                .float()\n",
    "                .mean()\n",
    "            )\n",
    "\n",
    "            probe_log_probs = probe_out.log_softmax(-1)\n",
    "            probe_correct_log_probs = (\n",
    "                einops.reduce(\n",
    "                    probe_log_probs * state_stack_one_hot,\n",
    "                    \"modes batch pos rows cols options -> modes pos rows cols\",\n",
    "                    \"mean\",\n",
    "                )\n",
    "                * one_hot_range\n",
    "            )  # Multiply to correct for the mean over one_hot_range\n",
    "            loss = -probe_correct_log_probs[0, :].mean(0).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            if i % 100 == 0:\n",
    "                logger.info(\n",
    "                    f\"epoch {epoch}, batch {i}, acc {accuracy}, loss {loss}, lr {lr}\"\n",
    "                )\n",
    "                if wandb_logging:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"acc\": accuracy,\n",
    "                            \"loss\": loss,\n",
    "                            \"lr\": lr,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"iter\": current_iter,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            current_iter += batch_size\n",
    "\n",
    "    checkpoint = {\n",
    "        \"linear_probe\": linear_probe,\n",
    "        \"layer\": layer,\n",
    "        \"config_name\": config.linear_probe_name,\n",
    "        \"final_loss\": loss,\n",
    "        \"model_name\": model_name,\n",
    "        \"iters\": current_iter,\n",
    "        \"epochs\": epoch,\n",
    "        \"acc\": accuracy,\n",
    "        \"dataset_prefix\": DATASET_PREFIX,\n",
    "        \"process_data\": PROCESS_DATA,\n",
    "        \"column_name\": config.column_name,\n",
    "        \"pos_start\": pos_start,\n",
    "        \"split\": SPLIT,\n",
    "        \"levels_of_interest\": levels_of_interest,\n",
    "        \"indexing_function_name\": indexing_function_name,\n",
    "        \"wandb_project\": wandb_project,\n",
    "        \"wandb_run_name\": wandb_run_name,\n",
    "    }\n",
    "    torch.save(checkpoint, linear_probe_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_function = MSELoss()\n",
    "mae_loss_function = L1Loss()\n",
    "\n",
    "def train_linear_probe_mse(layer: int):\n",
    "    linear_probe_name = f\"{PROBE_DIR}{model_name}_{config.linear_probe_name}_layer_{layer}.pth\"\n",
    "    linear_probe = torch.randn(\n",
    "        modes, model.cfg.d_model, config.num_rows, config.num_cols, requires_grad=False, device=device\n",
    "    )/np.sqrt(model.cfg.d_model)\n",
    "    linear_probe.requires_grad = True\n",
    "    logger.info(f\"linear_probe shape: {linear_probe.shape}\")\n",
    "    lr = max_lr\n",
    "    optimiser = torch.optim.AdamW([linear_probe], lr=lr, betas=(0.9, 0.99), weight_decay=wd)\n",
    "\n",
    "    logger.info(f\"custom_indices shape: {custom_indices.shape}\")\n",
    "\n",
    "    # logger.debug(dots_indices.shape)\n",
    "\n",
    "\n",
    "    if wandb_logging:\n",
    "        import wandb\n",
    "\n",
    "        wandb_project = \"chess_linear_probes_mse\"\n",
    "        wandb_run_name = f\"{config.linear_probe_name}_{model_name}_layer_{layer}_indexing_{indexing_function_name}\"\n",
    "        if PROCESS_DATA:\n",
    "            wandb_run_name += \"_levels\"\n",
    "            for level in levels_of_interest:\n",
    "                wandb_run_name += f\"_{level}\"\n",
    "\n",
    "        logging_dict = {\"linear_probe_name\": config.linear_probe_name, \"model_name\": model_name, \"layer\": layer,\n",
    "                        \"indexing_function_name\": indexing_function_name,\n",
    "                        \"batch_size\": batch_size, \"max_lr\": max_lr, \"wd\": wd, \"pos_start\": pos_start,\n",
    "                        \"num_epochs\": num_epochs, \"num_games\": num_games, \"modes\": modes,\n",
    "                        \"one_hot_range\": one_hot_range, \"wandb_project\": wandb_project, \"wandb_run_name\": wandb_run_name}\n",
    "        wandb.init(project=wandb_project, name=wandb_run_name, config=logging_dict)\n",
    "\n",
    "\n",
    "    current_iter = 0\n",
    "    loss = 0\n",
    "    acc_blank = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        full_train_indices = torch.randperm(num_games)\n",
    "        for i in tqdm(range(0, num_games, batch_size)):\n",
    "\n",
    "            lr = get_lr(current_iter, max_iters, max_lr, min_lr) if decay_lr else lr\n",
    "            for param_group in optimiser.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            \n",
    "            indices = full_train_indices[i:i+batch_size]\n",
    "            # logger.debug(\"indices\", indices)\n",
    "            list_of_indices = indices.tolist() # For indexing into the board_seqs_string list of strings\n",
    "            # logger.debug(list_of_indices)\n",
    "            games_int = board_seqs_int[indices]\n",
    "            games_int = games_int[:, :]\n",
    "            # logger.debug(games_int.shape)\n",
    "            games_str = [board_seqs_string[idx] for idx in list_of_indices]\n",
    "            games_str = [s[:] for s in games_str]\n",
    "            # logger.debug(games_str)\n",
    "            games_dots = custom_indices[indices]\n",
    "            games_dots = games_dots[:, pos_start:]\n",
    "            # logger.debug(games_dots)\n",
    "            # logger.debug(games_dots.shape)\n",
    "\n",
    "            if config.probing_for_skill:\n",
    "                games_skill = skill_stack[indices]\n",
    "                # logger.debug(\"GAMES SKILL\", games_skill)\n",
    "                # logger.debug(games_skill.shape)\n",
    "            else:\n",
    "                games_skill = None\n",
    "\n",
    "            # logger.debug(\"Games skill\", games_skill)\n",
    "\n",
    "            state_stack = chess_utils.create_state_stacks(games_str, config.custom_function, games_skill)\n",
    "            # logger.debug(\"STATE STACK\", state_stack)\n",
    "            # state_stack = state_stack[:, pos_start:pos_end, :, :]\n",
    "            # logger.debug(\"Shape before indexing state stack:\", state_stack.shape)\n",
    "            # Initialize a list to hold the indexed state stacks\n",
    "            indexed_state_stacks = []\n",
    "\n",
    "            for batch_idx in range(batch_size): # TODO FIX Batching\n",
    "                # Get the indices for the current batch\n",
    "                dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                # Index the state_stack for the current batch\n",
    "                indexed_state_stack = state_stack[:, batch_idx, dots_indices_for_batch, :, :]\n",
    "\n",
    "                # Append the result to the list\n",
    "                indexed_state_stacks.append(indexed_state_stack)\n",
    "\n",
    "            # Stack the indexed state stacks along the first dimension\n",
    "            # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "            # logger.debug(\"Length of indexed state stacks\", len(indexed_state_stacks))\n",
    "            state_stack = torch.stack(indexed_state_stacks).to(device)\n",
    "            state_stack = state_stack.to(dtype=torch.float32)\n",
    "            \n",
    "            # Use einops to rearrange the dimensions after stacking\n",
    "            state_stack = einops.rearrange(state_stack, 'batch modes pos row col -> modes batch pos row col')\n",
    "\n",
    "            # logger.debug(\"state stack\", state_stack)\n",
    "\n",
    "            # logger.debug(\"after indexing state stack shape\", state_stack.shape)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                _, cache = model.run_with_cache(games_int.to(device)[:, :-1], return_type=None)\n",
    "                resid_post = cache[\"resid_post\", layer][:, :]\n",
    "            # Initialize a list to hold the indexed state stacks\n",
    "            indexed_resid_posts = []\n",
    "\n",
    "            for batch_idx in range(games_dots.size(0)):\n",
    "                # Get the indices for the current batch\n",
    "                dots_indices_for_batch = games_dots[batch_idx]\n",
    "\n",
    "                # Index the state_stack for the current batch\n",
    "                indexed_resid_post = resid_post[batch_idx, dots_indices_for_batch]\n",
    "\n",
    "                # Append the result to the list\n",
    "                indexed_resid_posts.append(indexed_resid_post)\n",
    "\n",
    "            # Stack the indexed state stacks along the first dimension\n",
    "            # This results in a tensor of shape [2, 61, 8, 8] (assuming all batches have 61 indices)\n",
    "            resid_post = torch.stack(indexed_resid_posts)\n",
    "            # logger.debug(\"Resid post\", resid_post.shape)\n",
    "            probe_out = einsum(\n",
    "                \"batch pos d_model, modes d_model rows cols -> modes batch pos rows cols\",\n",
    "                resid_post,\n",
    "                linear_probe,\n",
    "            )\n",
    "            # logger.debug(probe_out.shape)\n",
    "\n",
    "            assert probe_out.shape == state_stack.shape\n",
    "\n",
    "            # acc_blank = (probe_out[0].argmax(-1) == state_stack_one_hot[0].argmax(-1)).float().mean()\n",
    "            \n",
    "            # logger.debug(probe_out.shape, state_stack.shape)\n",
    "            # logger.debug(probe_out[0][0][0][0][0])\n",
    "            # logger.debug(state_stack[0][0][0][0][0])\n",
    "            mse_loss = mse_loss_function(probe_out, state_stack)\n",
    "            mae_loss = mae_loss_function(probe_out, state_stack)\n",
    "            \n",
    "            mae_loss_denormalized = mae_loss * std\n",
    "\n",
    "            # logger.debug(probe_out.shape, probe_out.dtype)\n",
    "            # logger.debug(state_stack.shape, state_stack.dtype)\n",
    "\n",
    "\n",
    "            if i % 200 == 0:\n",
    "                logger.info(f\"epoch {epoch}, batch {i}, mae loss {mae_loss.item()}, mae denorm loss {mae_loss_denormalized.item()}, mse loss {mse_loss.item()}, lr {lr}\")\n",
    "                if wandb_logging:\n",
    "                    wandb.log({\"acc\": mae_loss.item(),\n",
    "                            \"loss\": mse_loss.item(),\n",
    "                            \"mae_denorm_loss\": mae_loss_denormalized.item(),\n",
    "                            \"lr\": lr,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"iter\": current_iter})\n",
    "\n",
    "            loss = mse_loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimiser.step()\n",
    "            optimiser.zero_grad()\n",
    "            current_iter += batch_size\n",
    "\n",
    "    checkpoint = {\n",
    "        \"linear_probe\": linear_probe,\n",
    "        \"layer\": layer,\n",
    "        \"config_name\": config.linear_probe_name,\n",
    "        \"final_loss\": loss,\n",
    "        \"model_name\": model_name,\n",
    "        \"iters\": current_iter,\n",
    "        \"epochs\": epoch,\n",
    "        \"acc\": acc_blank,\n",
    "        \"dataset_prefix\": DATASET_PREFIX,\n",
    "        \"process_data\": PROCESS_DATA,\n",
    "        \"column_name\": config.column_name,\n",
    "        \"pos_start\": pos_start,\n",
    "        \"split\": SPLIT,\n",
    "        \"levels_of_interest\": levels_of_interest,\n",
    "        \"indexing_function_name\": indexing_function_name,\n",
    "        \"wandb_project\": wandb_project,\n",
    "        \"wandb_run_name\": wandb_run_name,\n",
    "    }\n",
    "    torch.save(checkpoint, linear_probe_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, n_layers, 2):\n",
    "    if TRAIN_WITH_MSE:\n",
    "        logger.info(f\"Training linear probe with MSE loss on layer {i}\")\n",
    "        train_linear_probe_mse(i)\n",
    "    else:\n",
    "        logger.info(f\"Training linear probe with cross entropy loss on layer {i}\")\n",
    "        train_linear_probe_cross_entropy(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
