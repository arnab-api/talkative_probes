{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.dataset_manager as dataset_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['geometry_of_truth', 'relations', 'sst2', 'md_gender', 'snli', 'ag_news', 'ner', 'tense', 'language_identification', 'singular_plural'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.DatasetManager.list_datasets_by_group().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_of_truth_dataset = dataset_manager.DatasetManager.from_dataset_group(\"geometry_of_truth\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6478"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geometry_of_truth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Yaroslavl is in Russia.', questions=['# Do the facts support this statement?', '# Do you find this to be truthful?', '# May I assume this is correct?', '# Are these facts correct?', '# Do you think this checks out?', '# Is this true?', '# Should I believe this statement?', '# Can you confirm the accuracy of this?', '# Does this information ring true?', '# Does this statement hold true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='King Henry VIIIs only surviving legitimate child to survive childhood was Queen Elizabeth.', questions=['# Are you saying this is correct?', '# Can we confirm this is accurate?', '# Has this been verified as true?', '# Would you say this is accurate?', '# Could you tell me if this is factual?', '# Have I understood this correctly?', '# Are these facts correct?', '# Can you confirm the accuracy of this?', '# Is this statement true?', '# Am I right to believe this?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(geometry_of_truth_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = geometry_of_truth_dataset.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4535 1943\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_dataset = dataset_manager.DatasetManager.from_named_datasets([(\"geometry_of_truth\", \"cities\"), (\"geometry_of_truth\", \"neg_cities\")], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Kaduna is in Nigeria.', questions=['# Can you verify if this is true?', '# Would you consider this to be true?', '# Is this true?', '# Would you say this is accurate?', '# Does this information ring true?', '# Could you tell me if this is factual?', '# Should I believe this statement?', '# May I assume this is correct?', '# Are these facts correct?', '# Has this been verified as true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of San Miguel de Tucuman is in Argentina.', questions=['# Can we confirm this is accurate?', '# May I assume this is correct?', '# Are you saying this is correct?', '# Would you consider this to be true?', '# Are these facts correct?', '# Can you verify if this is true?', '# Has this been verified as true?', '# Am I right to believe this?', '# Can you confirm the accuracy of this?', '# Do the facts support this statement?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Nanded is in India.', questions=['# Has this been verified as true?', '# Are these facts correct?', '# Does this statement hold true?', '# Do you agree that this is true?', '# Should I believe this statement?', '# Have I understood this correctly?', '# Does this information ring true?', '# May I assume this is correct?', '# Do you find this to be truthful?', '# Can you confirm the accuracy of this?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Banjarmasin is not in Indonesia.', questions=['# Would you say this is accurate?', '# Are these facts correct?', '# Does this information ring true?', '# Is this statement true?', '# Are you confirming this is true?', '# Do you think this checks out?', '# Could you tell me if this is factual?', '# Can we confirm this is accurate?', '# Do you agree that this is true?', '# Would you consider this to be true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(named_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dataset = dataset_manager.DatasetManager.from_dataset_group(\n",
    "    \"relations\",\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Abu Ghosh is in the country of Israel.', questions=['# May I assume this is correct?', '# Has this been verified as true?', '# Is this statement true?', '# Does this information ring true?', '# Is this true?', '# Can you verify if this is true?', '# Could you tell me if this is factual?', '# Do you think this checks out?', '# Should I believe this statement?', '# Do the facts support this statement?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Selim Bouadla plays in the position of a midfielder.', questions=['# Would you validate this statement?', '# Has this been verified as true?', '# Do you find this to be truthful?', '# Can you verify if this is true?', '# Are you confirming this is true?', '# Would you consider this to be true?', '# Do you agree that this is true?', '# Are you saying this is correct?', '# Should I believe this statement?', '# Can we confirm this is accurate?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Lahti is in the country of Romania.', questions=['# Do you find this to be truthful?', '# Would you consider this to be true?', '# Does this information ring true?', '# Has this been verified as true?', '# Are you confirming this is true?', '# Does this statement hold true?', '# Do the facts support this statement?', '# Do you think this checks out?', '# Can you confirm the accuracy of this?', '# Can we confirm this is accurate?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Kosi Zone is in the country of Nepal.', questions=['# Would you validate this statement?', '# Should I believe this statement?', '# Would you consider this to be true?', '# Would you say this is accurate?', '# Is this true?', '# Do you agree that this is true?', '# Could you tell me if this is factual?', '# Can you confirm the accuracy of this?', '# Can you verify if this is true?', '# Are you confirming this is true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(relation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = dataset_manager.DatasetManager.from_dataset_group(\"sst2\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='sticks rigidly to the paradigm ,', questions=['# Is the general sentiment of this review positive?', '# Is the sentiment in this review on the positive side?', '# This review is negative. Do you agree?', '# Would you say this is an unfavorable review?', '# Does this review indicate a good experience?', '# Does this review lean towards the negative end of the spectrum?', '# Based on the language used, is this a negative review?', '# Are the comments in this review predominantly negative?', '# Are the comments in this review predominantly positive?', '# Would you say this is a favorable review?'], answers=['No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='keep you watching ,', questions=['# Is the general sentiment of this review negative?', '# This review is positive. Do you agree?', '# Can we consider this a thumbs-down review?', '# Does this review indicate a good experience?', '# Is the general sentiment of this review positive?', '# Are the comments in this review predominantly positive?', '# Is this a positive review?', '# Is there a negative tone throughout this review?', '# Does this review lean towards the negative end of the spectrum?', '# Would you categorize this as a negative piece of feedback?'], answers=['No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No']),\n",
       " ContextQASample(context=\"this remake of lina wertmuller 's 1975 eroti-comedy might just be the biggest husband-and-wife disaster since john and bo derek made the ridiculous bolero\", questions=['# Is the general sentiment of this review positive?', '# Does the reviewer appear displeased with their experience?', '# Reading between the lines, is this a positive review?', '# This review is positive. Do you agree?', '# Did the reviewer express satisfaction?', '# Does this review reflect a positive experience?', '# Does this review indicate a poor experience?', '# Can we interpret this as a positive evaluation?', '# Would you say this is a favorable review?', '# This review is negative. Do you agree?'], answers=['No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='rated eee for excitement', questions=['# Is the overall message of this review positive?', '# Does the reviewer appear displeased with their experience?', '# Did the reviewer have good things to say?', '# Does this review indicate a good experience?', '# Does the reviewer appear pleased with their experience?', '# This review is negative. Do you agree?', '# Is there a positive tone throughout this review?', '# Can we consider this a thumbs-up review?', '# Are the comments in this review predominantly positive?', '# Would you categorize this as a positive piece of feedback?'], answers=['Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sst2_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataset = dataset_manager.DatasetManager.from_dataset_group(\"md_gender\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context=\"It's important to take a deep breath and focus when piloting because every second counts, the Superbird-B1 is no exception.\\n\\nThis text is about Superbird-B1.\", questions=['# Am I learning about a female individual here?', '# Am I reading about a male person here?', '# Am I reading about someone who identifies as female?', '# Does this text feature a female person?', '# Is this talking about a female person?', '# Is the character described here male?', '# Does this portray a male individual?', '# Is this narrative about a female individual?', '# Does this refer to a male individual?', '# Is the focus of this sentence on someone male?'], answers=['No', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Davide Plebani made Italy proud when he competed at the 2016 UEC European Track Championships in the elimination race event.\\n\\nThis text is about Davide Plebani.', questions=['# Am I reading about someone who identifies as male?', '# Does this text feature a male person?', '# Is this about someone who is male?', '# Does this refer to a female individual?', '# Does this portray a male individual?', '# Is the subject of this sentence male?', '# Does this text discuss a male individual?', '# Is the focus of this sentence on someone female?', \"# Is this statement about someone who's female?\", '# Is the character described here female?'], answers=['Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No']),\n",
       " ContextQASample(context='John Dankworth and his wife teach marvelous music together! and he serves as her director of music as well!\\n\\nThis text is about John Dankworth.', questions=['# Am I reading about a male person here?', '# Am I learning about a female individual here?', '# Does this describe someone who is female?', '# Does this content reference a female person?', '# Is this narrative about a male individual?', '# Is this describing a male person?', \"# Is this statement about someone who's male?\", '# Does this portray a female individual?', '# Is the focus of this sentence on someone male?', '# Am I reading about someone who identifies as male?'], answers=['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Margie performed an amazing version of the Supremes\\'s \"Stop!.\"\\n\\nThis text is about Margie Joseph.', questions=[\"# Is this statement about someone who's male?\", '# Am I reading about someone who identifies as male?', '# Am I reading about a female person here?', '# Is the character described here female?', '# Does this describe someone who is male?', '# Does this portray a female individual?', '# Does this content reference a male person?', '# Am I learning about a female individual here?', '# Is the focus of this sentence on someone female?', '# Does this passage concern a male person?'], answers=['No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(gender_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2229"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = dataset_manager.DatasetManager.from_dataset_group(\"ag_news\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Hendrick Motorsports plane crashes, killing 10 on way to NASCAR &lt;b&gt;...&lt;/b&gt;\\n\\nA Hendrick Motorsports plane crashed Sunday on its way to a NASCAR race, killing all 10 people aboard, including the son, brother and two nieces of the owner of one of auto racing #39;s most successful organizations.', questions=['# Can this be considered a Sports-related article?', '# In terms of content, does this piece deal with World News?', '# Does this article primarily address Science/Technology?', '# Can this article be categorized under World News?', '# In essence, is this a piece about Sports?', '# Is Sports a key subject in this piece?', '# To what degree does this article cover World News?', '# Does the scope of this article encompass Business?', '# Is there a substantial focus on World News in this article?', '# Does this writing concentrate on aspects of Sports?'], answers=['Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='Infocus: Valuing Secure Access to Personal Information\\n\\nThis article seeks to answer the question: is your personal data safe? Or do you give it away during almost every transaction you make with government or commercial entities?', questions=['# Does this writing concentrate on aspects of Science/Technology?', '# Would you describe this as a discussion of Science/Technology?', '# To what degree does this article cover Sports?', '# Can this be considered a Business-related article?', '# Would you say this piece focuses on Science/Technology?', '# Could this be summarized as an article about World News?', '# Would you classify this as an article about World News?', '# Does this text explore Business?', '# Does this article primarily address Science/Technology?', '# Is there a substantial focus on Business in this article?'], answers=['Yes', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No']),\n",
       " ContextQASample(context='Astros, Cardinals Tied 4-4 After Five\\n\\nSt. Louis Cardinals #39; Albert Pujols hits a two-run homer in the first inning to tie the game against the Houston Astros during Game 1 of the National League Championship Series at Busch Stadium in St.', questions=['# Would you describe this as a discussion of World News?', '# In terms of content, does this piece deal with Sports?', '# Does the scope of this article encompass Sports?', '# Could this be summarized as an article about Sports?', '# Is Business a key subject in this piece?', '# Is there a substantial focus on World News in this article?', '# In essence, is this a piece about Sports?', '# How relevant is this article to Business?', '# Does this article delve into matters concerning World News?', '# Is Sports the central theme of this piece?'], answers=['No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='Talks on Airbus subsidies hit snags\\n\\nBee Washington Bureau. WASHINGTON - Trying to avert a trans-Atlantic trade war over government subsidies to Airbus, US and European negotiators found little to agree on Thursday during a five-hour meeting in Brussels, Belgium.', questions=['# Does this article delve into matters concerning Business?', '# How relevant is this article to Sports?', '# Would you describe this as a discussion of Sports?', '# Would you classify this as an article about Science/Technology?', '# Is Sports the main subject matter of this article?', '# Does this writing concentrate on aspects of Business?', '# Does the content of this article pertain to Business?', '# Can this be considered a Business-related article?', '# Would you say this piece focuses on Sports?', '# Can this article be categorized under Business?'], answers=['Yes', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(news_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NER dataset\n",
      "Processing NER dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 47959/47959 [00:21<00:00, 2266.67it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_dataset = dataset_manager.DatasetManager.from_dataset_group(\"ner\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Iran insists its nuclear program is used for the peaceful purpose of energy generation and has said it will soon resume uranium processing .', questions=['# Does this passage make any reference to Malay Peninsula?', '# Can you find any references to Port Elizabeth in this text?', '# Does this text contain the term Iran anywhere?', '# Are there any occurrences of Sudan Said Djinnit within this content?', '# Does this document contain any mentions of Iran?', '# Does this text mention Iran?', '# Is there any mention of Iran throughout this material?', '# Does this text have any instances of Iran?', '# Is Project Venezuela mentioned anywhere in the passage?', '# Is Dodger Stadium noted anywhere within these paragraphs?'], answers=['No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='At least five people have died from bird flu in China this year .', questions=['# Does this text contain the term Kilju anywhere?', '# Is China cited in this piece?', '# Are there any occurrences of Hellenic within this content?', '# Does this text have any instances of State for African Affairs?', '# Is China in any part of this passage?', '# Are there any references to China in this document?', '# Does this text refer to China?', '# Can you find any references to China in this text?', '# Does this writing include any mention of Celsius?', '# Can you detect China in this text sample?'], answers=['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes']),\n",
       " ContextQASample(context='NATO says after the joint force used a bullhorn to call for occupants of a vehicle to \" exit peacefully , \" a man got out of the vehicle with an AK-47 .', questions=['# Can you find any references to NATO in this text?', '# Does Raul show up anywhere in this text?', '# Does this text refer to NATO?', '# Is October , 2003 discussed anywhere in this content?', '# Does this document contain any mentions of Llodra?', '# Does this passage make any reference to United States and European Union?', '# Does this text contain the term Tanith Belbin anywhere?', '# Can you detect The Breakthrough in this text sample?', '# Is Republican Congressman referenced in this text?', '# Is downtown Beirut cited in this piece?'], answers=['Yes', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='An audio recording purported to be of the leader of al-Qaida in Iraq has been posted on the Internet , four days after reports he had been killed in a clash among members of his insurgent group .', questions=['# Can you identify any mentions of al-Qaida in this piece?', '# Does this text have any instances of first 9?', '# Is Iraq referenced in this text?', '# Does Agnes Uwimana show up anywhere in this text?', '# Are there any references to al-Qaida in this document?', '# Can you detect al-Qaida in this text sample?', '# Does this text mention Lieutenant General Ethem Erdagi?', '# Does this writing include any mention of al-Qaida?', '# Does this text contain the term Iraq anywhere?', '# Does this passage make any reference to al-Qaida?'], answers=['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ner_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SNLI dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 550152/550152 [00:26<00:00, 20392.10it/s]\n"
     ]
    }
   ],
   "source": [
    "snli_dataset = dataset_manager.DatasetManager.from_dataset_group(\"snli\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='A soccer game, with a huge crowd in the background.', questions=['# Would you say these two sentences connect logically? People playing soccer in front of a crowd.', '# Is there logical continuity between these sentences? People playing soccer in front of a crowd.', '# After reading the first sentence, is the second one coherent? People playing soccer in front of a crowd.', '# Would the following sentence align with what we just read? People playing soccer in front of a crowd.', '# Does this next sentence fit with the preceding context? People playing soccer in front of a crowd.', '# Looking at the previous context, does this next part hold up? People playing soccer in front of a crowd.', '# Following that statement, does this next part flow naturally? People playing soccer in front of a crowd.', '# Building on that last sentence, does this next one make sense? People playing soccer in front of a crowd.', '# Based on what was just stated, is this a reasonable follow-up? People playing soccer in front of a crowd.', '# With that setup, does the following sentence work? People playing soccer in front of a crowd.'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='A man is scratching his back while walking along the shore.', questions=['# Based on what was just stated, is this a reasonable follow-up? The man is standing next to water.', '# With that setup, does the following sentence work? The man is standing next to water.', '# Building on that last sentence, does this next one make sense? The man is standing next to water.', '# After reading the first sentence, is the second one coherent? The man is standing next to water.', '# Does this next sentence fit with the preceding context? The man is standing next to water.', '# Would the following sentence align with what we just read? The man is standing next to water.', '# Is the next sentence logically consistent with what came before? The man is standing next to water.', '# Looking at the previous context, does this next part hold up? The man is standing next to water.', '# Is there logical continuity between these sentences? The man is standing next to water.', '# Following that statement, does this next part flow naturally? The man is standing next to water.'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='A man with a cowboy hat, cane and suspenders walks down a country road.', questions=['# Following that statement, does this next part flow naturally? A man in a hat walking down a road.', '# After reading the first sentence, is the second one coherent? A man in a hat walking down a road.', '# Is there logical continuity between these sentences? A man in a hat walking down a road.', '# Would the following sentence align with what we just read? A man in a hat walking down a road.', '# Building on that last sentence, does this next one make sense? A man in a hat walking down a road.', '# With that setup, does the following sentence work? A man in a hat walking down a road.', '# Looking at the previous context, does this next part hold up? A man in a hat walking down a road.', '# Is the next sentence logically consistent with what came before? A man in a hat walking down a road.', '# Based on what was just stated, is this a reasonable follow-up? A man in a hat walking down a road.', '# Does this next sentence fit with the preceding context? A man in a hat walking down a road.'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Children enjoy an abundance of bubbles coming from a nearby fountain while someone captures the moment.', questions=['# Is there logical continuity between these sentences? Children are fast asleep during nap time.', '# Based on what was just stated, is this a reasonable follow-up? Children are fast asleep during nap time.', '# Building on that last sentence, does this next one make sense? Children are fast asleep during nap time.', '# Is the next sentence logically consistent with what came before? Children are fast asleep during nap time.', '# After reading the first sentence, is the second one coherent? Children are fast asleep during nap time.', '# Would the following sentence align with what we just read? Children are fast asleep during nap time.', '# Following that statement, does this next part flow naturally? Children are fast asleep during nap time.', '# Does this next sentence fit with the preceding context? Children are fast asleep during nap time.', '# Looking at the previous context, does this next part hold up? Children are fast asleep during nap time.', '# Would you say these two sentences connect logically? Children are fast asleep during nap time.'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(snli_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tense_dataset = dataset_manager.DatasetManager.from_dataset_group(\"tense\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context=\"In a week's time, she will have been working on her thesis for a year.\", questions=['# Would you identify this as an example of the present tense?', '# Is the time frame of this statement future?', '# Is the narrative presented in the present tense?', '# Does this sentence convey the past tense?', '# Am I correct in saying this is in the present tense?', '# Is the narrative presented in the past tense?', '# This statement is in the present tense. Do you agree?', '# Can we classify this sentence as being in the future tense?', '# Can we classify this sentence as being in the future tense?', '# Would you say this is written in the future tense?'], answers=['No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='She has rescued a stray dog.', questions=['# Would you identify this as an example of the future tense?', '# Would you say this is written in the future tense?', '# Is the structure of this sentence indicative of the future tense?', '# Is the narrative presented in the future tense?', '# Does this sentence convey the future tense?', '# Is the action taking place in the present tense?', '# Is the action described here in the future tense?', '# This statement is in the past tense. Do you agree?', '# Is the action taking place in the future tense?', '# Is the narrative presented in the future tense?'], answers=['No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Had you ever explored such a unique destination before', questions=['# Is the narrative presented in the past tense?', '# Does this sentence align with the past tense?', '# Is the time frame of this statement future?', '# Am I correct in saying this is in the past tense?', '# Does this text reflect the future tense?', '# Is the action described here in the future tense?', '# Is this statement in the past tense?', '# Is the verb form in this sentence past?', '# Is the action described here in the past tense?', '# Would you agree that this is in the past tense?'], answers=['Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='They will be hosting a fundraising gala for a non-profit organization.', questions=['# Can we classify this sentence as being in the future tense?', '# Is the action taking place in the future tense?', '# Is the action taking place in the present tense?', '# Is the verb form in this sentence present?', '# This statement is in the present tense. Do you agree?', '# Is this statement in the future tense?', '# Does this sentence align with the present tense?', '# Is the action described here in the future tense?', '# This statement is in the past tense. Do you agree?', '# Is the action taking place in the future tense?'], answers=['Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes'])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(tense_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1687"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tense_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced3e8fa2eda408db9dae499996d3282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c5191e4d2f4815af985e3f0ab9d4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/13.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdc07a7083e46529e233c12444a520d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/22000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "language_dataset = dataset_manager.DatasetManager.from_dataset_group(\"language_identification\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='moyle p y j cech fishes an introduction to ichthyology a edición upper saddle river nueva jersey estados unidos prentice-hall año ', questions=['# Am I correct in saying this is in Spanish?', '# Is the text presented in Urdu?', '# Am I correct in saying this is in Spanish?', '# Would you agree that this is in Thai?', '# Would you classify this as written in Spanish?', '# Can we confirm this is in Spanish?', '# Is the language used here Spanish?', '# Can we determine that this is in Turkish?', '# Would you agree that this is in Romanian?', '# Does this passage use Hindi?'], answers=['Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No']),\n",
       " ContextQASample(context='ในปี คศ  ไบเยวูและทัคเกอร์กลับมารวมตัวกันอีกครั้ง และออกทัวร์คอนเสิร์ตในสหราชอาณาจักรและไอร์แลนด์ ในเดือนกุมภาพันธ์และมีนาคม คศ ', questions=['# Is the text presented in Korean?', '# Is the text presented in Turkish?', '# Is the language of this text Thai?', '# Would you agree that this is in Thai?', '# Is this text predominantly in Thai?', '# Is this text predominantly in Chinese?', '# Is the primary language of this text Romanian?', '# Is this passage composed in Russian?', '# Would you identify this text as being in Chinese?', '# Would you identify this text as being in Portugese?'], answers=['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context=' शीतकालीन ओलंपिक में बैथलॉन रूस के क्रस्नाया पोलानाना के पास लौरा बायथलॉन और स्की कॉम्प्लेक्स में आयोजित किया गया था। ग्यारह घटनाओं - फरवरी  के बीच हुईं।', questions=['# Is this passage composed in English?', '# Is the content of this text in Dutch?', '# Is this written in the language Swedish?', '# Would you identify this text as being in Hindi?', '# Is the language used here Hindi?', '# Is the language used here Hindi?', '# Is the primary language of this text Hindi?', '# Is the language used here Portugese?', '# Would you agree that this is in French?', '# Would you identify this text as being in Thai?'], answers=['No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No']),\n",
       " ContextQASample(context='наиболее полное представление ломоносова о геологии и преобразованиях лика земного содержится в его работе «о слоях земных» которую называют началом русской научной геологии ломоносовым выдвинута гипотеза о существовании зон с быстрыми и медленными вертикальными движениями земной тверди в зависимости от силы «внутреннего огня» о первостепенном вкладе этих движений в происхождение крупнейших неровностей земной поверхности', questions=['# Is this text written in Russian?', '# Is this text predominantly in Swedish?', '# Is the text presented in Arabic?', '# Would you agree that this is in Russian?', '# Does this writing reflect the language Turkish?', '# Is the content of this text in Persian?', '# Is this text predominantly in Russian?', '# Is this text written in English?', '# Is this text written in Russian?', '# Is this passage composed in Russian?'], answers=['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(language_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "singular_plural_dataset = dataset_manager.DatasetManager.from_dataset_group(\"singular_plural\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The drivers transported passengers.', questions=['# Is this statement about one entity?', '# Can we interpret this as referring to a single entity?', '# Is the reference here to more than one individual?', '# Is this statement about one entity?', '# Is the reference here to one individual?', '# Does this sentence focus on a single person?', '# Is this text discussing one individual?', '# Is the reference here to one individual?', '# Is the reference here to more than one individual?', '# Is this statement about one entity?'], answers=['No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No']),\n",
       " ContextQASample(context='Keith writes songs.', questions=['# Does this statement indicate a plural subject?', '# Is the reference here to more than one individual?', '# Does this sentence focus on a single person?', '# Is this text discussing one individual?', '# Is this sentence referring to one individual?', '# Is this statement about more than one entity?', '# Is this narrative about multiple subjects?', '# Am I correct in saying this is about multiple people?', '# Can we interpret this as referring to multiple entities?', '# Am I correct in saying this is about multiple people?'], answers=['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Martha designs clothes.', questions=['# Is the subject of this sentence more than one person?', '# Does this sentence focus on multiple people?', '# Does this passage concern multiple individuals?', '# Is the subject of this sentence more than one person?', '# Does this passage concern a single individual?', '# Is this narrative about a single subject?', '# Is this statement about more than one entity?', '# Is this narrative about multiple subjects?', '# Does this statement indicate a plural subject?', '# Is the reference here to one individual?'], answers=['No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='Stella writes letters.', questions=['# Am I correct in saying this is about multiple people?', '# Does this passage concern a single individual?', '# Does this sentence focus on multiple people?', '# Is this statement about more than one entity?', '# Can we interpret this as referring to multiple entities?', '# Is the character described here one person?', '# Does this statement indicate a plural subject?', '# Is this about more than one person?', '# Is the character described here more than one person?', '# Is this statement about one entity?'], answers=['No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes'])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(singular_plural_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(singular_plural_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
