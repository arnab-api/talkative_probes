{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.dataset_manager as dataset_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geometry_of_truth': ['sp_en_trans',\n",
       "  'cities',\n",
       "  'neg_cities',\n",
       "  'smaller_than',\n",
       "  'larger_than',\n",
       "  'common_claim_true_false',\n",
       "  'companies_true_false'],\n",
       " 'sst2': ['sst2'],\n",
       " 'relations': ['factual/person_plays_instrument',\n",
       "  'factual/person_plays_pro_sport',\n",
       "  'factual/superhero_person',\n",
       "  'factual/country_largest_city',\n",
       "  'factual/food_from_country',\n",
       "  'factual/person_plays_position_in_sport',\n",
       "  'factual/country_capital_city',\n",
       "  'factual/landmark_in_country',\n",
       "  'factual/country_language',\n",
       "  'factual/country_currency',\n",
       "  'factual/superhero_archnemesis',\n",
       "  'factual/city_in_country',\n",
       "  'commonsense/task_done_by_tool',\n",
       "  'commonsense/object_superclass',\n",
       "  'commonsense/fruit_outside_color',\n",
       "  'commonsense/work_location',\n",
       "  'commonsense/substance_phase',\n",
       "  'commonsense/task_done_by_person',\n",
       "  'commonsense/word_sentiment',\n",
       "  'linguistic/adj_antonym',\n",
       "  'linguistic/word_first_letter',\n",
       "  'linguistic/word_last_letter',\n",
       "  'linguistic/adj_comparative',\n",
       "  'linguistic/adj_superlative',\n",
       "  'linguistic/verb_past_tense']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.DatasetManager.list_datasets_by_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_of_truth_dataset = dataset_manager.DatasetManager.from_dataset_group(\"geometry_of_truth\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6478"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geometry_of_truth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Seventy-three is larger than sixty-three.', question='Do you agree that this is true?', answer='Yes'),\n",
       " ContextQASample(context='Dragonflies make good housekeepers.', question='Are you confirming this is true?', answer='No')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(geometry_of_truth_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = geometry_of_truth_dataset.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4535 1943\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_dataset = dataset_manager.DatasetManager.from_named_datasets([(\"geometry_of_truth\", \"cities\"), (\"geometry_of_truth\", \"neg_cities\")], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Korla is in Azerbaijan.', question='Is this true?', answer='No'),\n",
       " ContextQASample(context='The city of Battagram is not in Pakistan.', question='Do you agree that this is true?', answer='No'),\n",
       " ContextQASample(context='The city of San Luis Potosi is not in the Democratic Republic of the Congo.', question='Would you say this is accurate?', answer='Yes'),\n",
       " ContextQASample(context='The city of Phnom Penh is not in Senegal.', question='Could you tell me if this is factual?', answer='Yes')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(named_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dataset = dataset_manager.DatasetManager.from_dataset_group(\n",
    "    \"relations\",\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Reiherstieg Schiffswerfte & Maschinenfabrik is in the country of Germany.', question='May I assume this is correct?', answer='Yes'),\n",
       " ContextQASample(context='In their sport, Anthony Obodai plays as a catcher.', question='Are you saying this is correct?', answer='No'),\n",
       " ContextQASample(context='Julian Lennon plays the guitar.', question='Do the facts support this statement?', answer='Yes'),\n",
       " ContextQASample(context='Pandoh Dam is in the country of India.', question='Does this statement hold true?', answer='Yes')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(relation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = dataset_manager.DatasetManager.from_dataset_group(\"sst2\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='even as the hero of the story rediscovers his passion in life , the mood remains oddly detached .', question='Does this review indicate a good experience?', answer='No'),\n",
       " ContextQASample(context='rarely work in movies now', question='Looking at this review, was the person happy with their experience?', answer='No'),\n",
       " ContextQASample(context='is mostly a lump of run-of-the-mill profanity sprinkled with a few remarks so geared toward engendering audience sympathy that you might think he was running for office -- or trying to win over a probation officer', question='Would you categorize this as a positive piece of feedback?', answer='No'),\n",
       " ContextQASample(context='an exploration that is more accurate than anything i have seen in an american film', question='Did the reviewer express satisfaction?', answer='Yes')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sst2_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
