{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.dataset_manager as dataset_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geometry_of_truth': ['sp_en_trans',\n",
       "  'neg_sp_en_trans',\n",
       "  'cities',\n",
       "  'neg_cities',\n",
       "  'smaller_than',\n",
       "  'larger_than',\n",
       "  'common_claim_true_false',\n",
       "  'companies_true_false',\n",
       "  'counterfact_true_false'],\n",
       " 'sst2': ['sst2'],\n",
       " 'relations': ['factual/person_plays_instrument',\n",
       "  'factual/person_plays_pro_sport',\n",
       "  'factual/presidents_birth_year',\n",
       "  'factual/star_constellation',\n",
       "  'factual/person_native_language',\n",
       "  'factual/person_father',\n",
       "  'factual/superhero_person',\n",
       "  'factual/presidents_election_year',\n",
       "  'factual/person_university',\n",
       "  'factual/person_band_lead_singer',\n",
       "  'factual/product_by_company',\n",
       "  'factual/country_largest_city',\n",
       "  'factual/pokemon_evolutions',\n",
       "  'factual/food_from_country',\n",
       "  'factual/company_ceo',\n",
       "  'factual/landmark_on_continent',\n",
       "  'factual/person_plays_position_in_sport',\n",
       "  'factual/country_capital_city',\n",
       "  'factual/landmark_in_country',\n",
       "  'factual/person_mother',\n",
       "  'factual/person_occupation',\n",
       "  'factual/country_language',\n",
       "  'factual/country_currency',\n",
       "  'factual/superhero_archnemesis',\n",
       "  'factual/company_hq',\n",
       "  'factual/city_in_country',\n",
       "  'commonsense/task_done_by_tool',\n",
       "  'commonsense/object_superclass',\n",
       "  'commonsense/fruit_outside_color',\n",
       "  'commonsense/work_location',\n",
       "  'commonsense/substance_phase',\n",
       "  'commonsense/task_done_by_person',\n",
       "  'commonsense/word_sentiment',\n",
       "  'commonsense/fruit_inside_color',\n",
       "  'linguistic/adj_antonym',\n",
       "  'linguistic/word_first_letter',\n",
       "  'linguistic/word_last_letter',\n",
       "  'linguistic/adj_comparative',\n",
       "  'linguistic/adj_superlative',\n",
       "  'linguistic/verb_past_tense']}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.DatasetManager.list_datasets_by_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_of_truth_dataset = dataset_manager.DatasetManager.from_dataset_group(\"geometry_of_truth\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22637"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geometry_of_truth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextAndAnswers(context='Clement XII passed away at Paris.', correct='false', incorrect='true'),\n",
       " ContextAndAnswers(context='The city of Semarang is not in Saudi Arabia.', correct='true', incorrect='false')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(geometry_of_truth_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = geometry_of_truth_dataset.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15846 6791\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_dataset = dataset_manager.DatasetManager.from_named_datasets([(\"geometry_of_truth\", \"cities\"), (\"geometry_of_truth\", \"neg_cities\")], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextAndAnswers(context='The city of Can Tho is in Venezuela.', correct='false', incorrect='true'),\n",
       " ContextAndAnswers(context='The city of Bhilai is in India.', correct='true', incorrect='false'),\n",
       " ContextAndAnswers(context='The city of Seongnam-si is not in South Korea.', correct='false', incorrect='true'),\n",
       " ContextAndAnswers(context='The city of Pune is in India.', correct='true', incorrect='false')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(named_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dataset = dataset_manager.DatasetManager.from_dataset_group(\n",
    "    \"relations\",\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextAndAnswers(context='Nicola Sirkis speaks the language of Indonesian.', correct='false', incorrect='true'),\n",
       " ContextAndAnswers(context='Brialmont Cove is on the continent of Africa.', correct='false', incorrect='true'),\n",
       " ContextAndAnswers(context='Majorette is headquartered in the city of Lyon.', correct='true', incorrect='false'),\n",
       " ContextAndAnswers(context='People in France speak English.', correct='false', incorrect='true')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(relation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = dataset_manager.DatasetManager.from_dataset_group(\"sst2\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextAndAnswers(context='fresh air', correct='positive', incorrect='negative'),\n",
       " ContextAndAnswers(context='photographed and staged by mendes with a series of riveting set pieces', correct='positive', incorrect='negative'),\n",
       " ContextAndAnswers(context='is pretty funny now and then without in any way demeaning its subjects', correct='positive', incorrect='negative'),\n",
       " ContextAndAnswers(context='a serious movie with serious ideas .', correct='positive', incorrect='negative')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sst2_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
