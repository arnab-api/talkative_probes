{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.dataset_manager as dataset_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['geometry_of_truth', 'relations', 'sst2', 'md_gender', 'ag_news', 'ner'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.DatasetManager.list_datasets_by_group().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_of_truth_dataset = dataset_manager.DatasetManager.from_dataset_group(\"geometry_of_truth\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6478"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geometry_of_truth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context=\"The Spanish word 'empujar' means 'to govern'.\", questions=['# Do you agree that this is true?', '# Does this statement hold true?', '# May I assume this is correct?', '# Are these facts correct?', '# Has this been verified as true?', '# Do the facts support this statement?', '# Are you saying this is correct?', '# Should I believe this statement?', '# Does this information ring true?', '# Do you think this checks out?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Samarkand is in China.', questions=['# Do you think this checks out?', '# Are you confirming this is true?', '# Can we confirm this is accurate?', '# Is this true?', '# Can you confirm the accuracy of this?', '# Could you tell me if this is factual?', '# Are you saying this is correct?', '# Am I right to believe this?', '# Would you say this is accurate?', '# Does this information ring true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(geometry_of_truth_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = geometry_of_truth_dataset.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4535 1943\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_dataset = dataset_manager.DatasetManager.from_named_datasets([(\"geometry_of_truth\", \"cities\"), (\"geometry_of_truth\", \"neg_cities\")], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Davao is not in the Philippines.', questions=['# Can you verify if this is true?', '# Are you saying this is correct?', '# Do the facts support this statement?', '# Do you agree that this is true?', '# Does this information ring true?', '# Have I understood this correctly?', '# Would you validate this statement?', '# Are these facts correct?', '# Should I believe this statement?', '# Am I right to believe this?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Santo Domingo Este is in the Dominican Republic.', questions=['# Would you say this is accurate?', '# Am I right to believe this?', '# Is this true?', '# Do you agree that this is true?', '# Is this statement true?', '# Do you think this checks out?', '# Do you find this to be truthful?', '# Do the facts support this statement?', '# Are you saying this is correct?', '# Can you confirm the accuracy of this?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Can Tho is in Vietnam.', questions=['# Do the facts support this statement?', '# Am I right to believe this?', '# Are these facts correct?', '# Can we confirm this is accurate?', '# Could you tell me if this is factual?', '# May I assume this is correct?', '# Should I believe this statement?', '# Do you find this to be truthful?', '# Can you confirm the accuracy of this?', '# Would you validate this statement?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Samarinda is not in Indonesia.', questions=['# Would you say this is accurate?', '# Would you validate this statement?', '# Should I believe this statement?', '# Would you consider this to be true?', '# Can you verify if this is true?', '# Am I right to believe this?', '# Could you tell me if this is factual?', '# Are these facts correct?', '# Have I understood this correctly?', '# Are you saying this is correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(named_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dataset = dataset_manager.DatasetManager.from_dataset_group(\n",
    "    \"relations\",\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Mont Saint-Bruno is in the country of Kosovo.', questions=['# Are you confirming this is true?', '# Have I understood this correctly?', '# Are you saying this is correct?', '# Should I believe this statement?', '# Would you say this is accurate?', '# Do you find this to be truthful?', '# Does this statement hold true?', '# Do the facts support this statement?', '# Would you validate this statement?', '# Are these facts correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Tampines Expressway is in the country of Bangladesh.', questions=['# Do the facts support this statement?', '# Do you find this to be truthful?', '# Do you agree that this is true?', '# Would you say this is accurate?', '# Can you confirm the accuracy of this?', '# Has this been verified as true?', '# Should I believe this statement?', '# Could you tell me if this is factual?', '# Are you saying this is correct?', '# Can we confirm this is accurate?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='In their sport, Antonio Cabrini plays as a centre.', questions=['# Does this information ring true?', '# Are these facts correct?', '# Would you say this is accurate?', '# Would you validate this statement?', '# Has this been verified as true?', '# Do you find this to be truthful?', '# Can you verify if this is true?', '# Are you confirming this is true?', '# Should I believe this statement?', '# Have I understood this correctly?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Juan Fernando Quintero plays in the position of a forward.', questions=['# Do you find this to be truthful?', '# Are you confirming this is true?', '# Would you say this is accurate?', '# Does this statement hold true?', '# Are these facts correct?', '# Can you confirm the accuracy of this?', '# May I assume this is correct?', '# Is this true?', '# Should I believe this statement?', '# Are you saying this is correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(relation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = dataset_manager.DatasetManager.from_dataset_group(\"sst2\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='qualities that were once amusing are becoming irritating .', questions=['# Did the reviewer express dissatisfaction?', '# Does this review reflect a negative experience?', '# Based on the language used, is this a positive review?', '# Does this review convey customer dissatisfaction?', '# Can we interpret this as a critical evaluation?', '# Is the overall message of this review negative?', '# Does this review lean towards the negative end of the spectrum?', '# Would you say this is a favorable review?', '# Would you categorize this as a positive piece of feedback?', '# Is there a negative tone throughout this review?'], answers=['Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='of the more serious-minded concerns of other year-end movies', questions=['# Reading between the lines, is this a negative review?', '# Does this review express negative sentiment?', '# Is there a negative tone throughout this review?', '# Is the general sentiment of this review negative?', '# Is the overall message of this review positive?', '# Is the sentiment in this review on the negative side?', '# Based on the language used, is this a positive review?', '# Is the general sentiment of this review positive?', '# Is the sentiment in this review on the positive side?', '# Does the reviewer appear pleased with their experience?'], answers=['No', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context=\"we can tell what it is supposed to be , but ca n't really call it a work of art .\", questions=['# Reading between the lines, is this a positive review?', '# Based on the language used, is this a positive review?', '# Is the sentiment in this review on the negative side?', '# This review is positive. Do you agree?', '# Does this review indicate a good experience?', '# Does this review express positive sentiment?', '# Would you categorize this as a negative piece of feedback?', '# Does the reviewer appear displeased with their experience?', '# Does the reviewer appear pleased with their experience?', '# Does this review lean towards the negative end of the spectrum?'], answers=['No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes']),\n",
       " ContextQASample(context='orchestrating a finale that is impenetrable and dull', questions=['# Would you say this is a favorable review?', '# Did the reviewer express dissatisfaction?', '# Is there a negative tone throughout this review?', '# Looking at this review, was the person happy with their experience?', '# Does this review convey customer dissatisfaction?', '# Is there a positive tone throughout this review?', '# Is the general sentiment of this review negative?', '# Reading between the lines, is this a positive review?', '# Is this a negative review?', '# Does the reviewer appear pleased with their experience?'], answers=['No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sst2_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataset = dataset_manager.DatasetManager.from_dataset_group(\"md_gender\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='I am absolutely sure that Whitehall Apartments features scenes of Christopher Columbus.\\n\\nThis text is about Whitehall Apartments.', questions=['# Is this sentence about a male person?', '# Is the character described here female?', '# Does this passage concern a female person?', '# Am I reading about a female person here?', '# Does this text feature a male person?', '# Is this narrative about a female individual?', '# Does this describe someone who is female?', \"# Is this statement about someone who's male?\", '# Is this describing a female person?', '# Is the person mentioned female?'], answers=['Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='The creative creators named the comic strip and character Claire Voyant, which was a sly pun to the word \"clairvoyant\".\\n\\nThis text is about Claire Voyant (comic strip).', questions=['# Am I reading about a female person here?', '# Is this narrative about a male individual?', '# Does this passage concern a female person?', '# Is the focus of this sentence on someone female?', '# Does this portray a female individual?', '# Am I learning about a male individual here?', '# Does this describe someone who is male?', '# Does this refer to a male individual?', '# Does this text discuss a male individual?', '# Does this content reference a female person?'], answers=['Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='Anja Hajduk is from German and he is in to political.\\n\\nThis text is about Anja Hajduk.', questions=['# Is the character described here female?', '# Am I learning about a female individual here?', '# Is this about someone who is male?', '# Is this describing a male person?', '# Does this text discuss a male individual?', \"# Is this statement about someone who's female?\", '# Is this sentence about a male person?', '# Am I reading about someone who identifies as female?', '# Is the focus of this sentence on someone male?', '# Does this describe someone who is male?'], answers=['Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='The market correction was predicted long ago by Asia Squawk Box\\n\\nThis text is about Asia Squawk Box.', questions=['# Is this narrative about a male individual?', '# Does this text feature a male person?', '# Is the person mentioned male?', '# Is this sentence about a male person?', '# Does this text discuss a male individual?', \"# Is this statement about someone who's male?\", '# Am I reading about someone who identifies as male?', '# Is this describing a female person?', '# Does this describe someone who is female?', '# Does this portray a male individual?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(gender_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2229"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = dataset_manager.DatasetManager.from_dataset_group(\"ag_news\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Genesis capsule carrying solar dust crashes\\n\\nWASHINGTON : NASA #39;s Genesis capsule slammed into the Utah desert after its parachutes failed, leaving scientists unsure if they could retrieve the bits of solar dust captured on its three-year mission.', questions=['# Would you describe this as a discussion of Science/Technology?', '# Can this be considered a Business-related article?', '# In essence, is this a piece about Sports?', '# How relevant is this article to Science/Technology?', '# Does this article primarily address World News?', '# Does this article delve into matters concerning World News?', '# Can this article be categorized under Science/Technology?', '# Does this text explore Sports?', '# Does the scope of this article encompass Science/Technology?', '# In terms of content, does this piece deal with Science/Technology?'], answers=['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Uruguay poised to elect leftwinger\\n\\nTabare Vazquez was poised to become Uruguay #39;s first leftwing president yesterday after campaigning for the more equitable distribution of wealth and social justice in a country crippled by a recent economic crisis.', questions=['# Is Sports the central theme of this piece?', '# Does this article delve into matters concerning Sports?', '# Does this writing concentrate on aspects of World News?', '# Would you describe this as a discussion of World News?', '# Can this article be categorized under World News?', '# Does the scope of this article encompass World News?', '# To what degree does this article cover World News?', '# Does this text explore Business?', '# Is there a substantial focus on World News in this article?', '# Does this article primarily address Science/Technology?'], answers=['No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No']),\n",
       " ContextQASample(context='From tiny acorns...\\n\\nIt was far from vintage Arsenal. The swagger and confidence of the opening month of the Premiership campaign was some distance away, but most importantly they began their Champions League campaign with three points.', questions=['# Is Sports the central theme of this piece?', '# Would you classify this as an article about Science/Technology?', '# Would you say this piece focuses on Sports?', '# To what degree does this article cover Sports?', '# Can this be considered a Sports-related article?', '# Does this article delve into matters concerning Sports?', '# Is Science/Technology a key subject in this piece?', '# Is Sports the main subject matter of this article?', '# Does the scope of this article encompass World News?', '# Does this writing concentrate on aspects of Business?'], answers=['Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='Tigers Romp Devil Rays 8-0 in Game 1 (AP)\\n\\nAP - Ivan Rodriguez hit his 250th career homer and Jeremy Bonderman threw a four-hitter to lead the Detroit Tigers past the Tampa Bay Devil Rays 8-0 in the opening game of a doubleheader Thursday.', questions=['# Does the content of this article pertain to Sports?', '# Does this article primarily address Sports?', '# Can this article be categorized under Business?', '# Can this be considered a Sports-related article?', '# Does the scope of this article encompass Sports?', '# Could this be summarized as an article about Sports?', '# Is Business the central theme of this piece?', '# Would you describe this as a discussion of Business?', '# Is Business the main subject matter of this article?', '# In essence, is this a piece about Sports?'], answers=['Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(news_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading NER dataset\n",
      "Processing NER dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 47959/47959 [00:25<00:00, 1849.48it/s]\n"
     ]
    }
   ],
   "source": [
    "ner_dataset = dataset_manager.DatasetManager.from_dataset_group(\"ner\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context=\"Also Tuesday , the United States said its warplanes conducted two strikes on buildings used by Abu Musab al-Zarqawi 's terror network in the western city of Fallujah .\", questions=['# Does this text mention United States?', '# Can you find any references to Rehman Malik in this text?', '# Is United States in any part of this passage?', '# Is Kaibyshev cited in this piece?', '# Does General Houghton show up anywhere in this text?', '# Does this text have any instances of Tuesday?', '# Is Aaron Galindo mentioned anywhere in the passage?', '# Is Fallujah present in this text?', '# Is Ali Shakeri noted anywhere within these paragraphs?', '# Does this writing include any mention of UN High Commissioner for Refugees?'], answers=['Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='In October , a Spanish photographer was kidnapped in the Gaza Strip .', questions=['# Is there any mention of Tandja Mamadou throughout this material?', '# Does this text contain the term Al Franken anywhere?', '# Is Srinagar discussed anywhere in this content?', '# Does this passage make any reference to Spanish?', '# Does this text mention Rajoelina?', '# Does this document contain any mentions of Gaza Strip?', \"# Is Sudan 's referenced in this text?\", '# Are there any references to Moscow Friday in this document?', '# Is Gaza Strip cited in this piece?', '# Does this text refer to 0.161?'], answers=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No']),\n",
       " ContextQASample(context=\"Insurgents in Iraq have carried out a series of attacks Tuesday north of Baghdad against the country 's security forces , killing at least 23 police and national guards .\", questions=['# Can you find any references to Black Sea in this text?', '# Are there any occurrences of Iraq within this content?', '# Are there any references to President Roh in this document?', '# Does this writing include any mention of Baghdad?', '# Is Tuesday in any part of this passage?', '# Does six-year term show up anywhere in this text?', '# Does this text mention Line Island?', '# Does Simonyi appear in this text?', '# Is there any mention of Minister Rodrigo Cabezas throughout this material?', '# Is Baghdad present in this text?'], answers=['No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context='The WHO says natural disasters , environmental change , bioterrorism and chemical spills also pose major challenges .', questions=['# Is 11 a.m. noted anywhere within these paragraphs?', '# Can you identify any mentions of Domenech in this piece?', '# Can you find any references to WHO in this text?', '# Does this text have any instances of WHO?', '# Is April 25 discussed anywhere in this content?', '# Does WHO show up anywhere in this text?', '# Is there any mention of WHO throughout this material?', '# Does this text refer to WHO?', '# Does WHO appear in this text?', '# Does this passage make any reference to Central African Republic?'], answers=['No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ner_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SNLI dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 550152/550152 [00:28<00:00, 19281.48it/s]\n"
     ]
    }
   ],
   "source": [
    "snli_dataset = dataset_manager.DatasetManager.from_dataset_group(\"snli\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='A welder dressed in green at his work station', questions=['# Based on what was just stated, is this a reasonable follow-up? A welder dressed in black at the bar.', '# After reading the first sentence, is the second one coherent? A welder dressed in black at the bar.', '# Would you say these two sentences connect logically? A welder dressed in black at the bar.', '# With that setup, does the following sentence work? A welder dressed in black at the bar.', '# Would the following sentence align with what we just read? A welder dressed in black at the bar.', '# Does this next sentence fit with the preceding context? A welder dressed in black at the bar.', '# Building on that last sentence, does this next one make sense? A welder dressed in black at the bar.', '# Following that statement, does this next part flow naturally? A welder dressed in black at the bar.', '# Looking at the previous context, does this next part hold up? A welder dressed in black at the bar.', '# Is there logical continuity between these sentences? A welder dressed in black at the bar.'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Two women laugh while sitting on the floor.', questions=['# After reading the first sentence, is the second one coherent? There are two men sitting on the couch.', '# Is there logical continuity between these sentences? There are two men sitting on the couch.', '# Would you say these two sentences connect logically? There are two men sitting on the couch.', '# Looking at the previous context, does this next part hold up? There are two men sitting on the couch.', '# Would the following sentence align with what we just read? There are two men sitting on the couch.', '# Is the next sentence logically consistent with what came before? There are two men sitting on the couch.', '# Following that statement, does this next part flow naturally? There are two men sitting on the couch.', '# Based on what was just stated, is this a reasonable follow-up? There are two men sitting on the couch.', '# Building on that last sentence, does this next one make sense? There are two men sitting on the couch.', '# With that setup, does the following sentence work? There are two men sitting on the couch.'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='An Asian girl performs a routine with two red flags in a square.', questions=['# Is the next sentence logically consistent with what came before? An Asian girl is performing a routine.', '# Does this next sentence fit with the preceding context? An Asian girl is performing a routine.', '# After reading the first sentence, is the second one coherent? An Asian girl is performing a routine.', '# Building on that last sentence, does this next one make sense? An Asian girl is performing a routine.', '# Based on what was just stated, is this a reasonable follow-up? An Asian girl is performing a routine.', '# Looking at the previous context, does this next part hold up? An Asian girl is performing a routine.', '# Would the following sentence align with what we just read? An Asian girl is performing a routine.', '# Would you say these two sentences connect logically? An Asian girl is performing a routine.', '# With that setup, does the following sentence work? An Asian girl is performing a routine.', '# Following that statement, does this next part flow naturally? An Asian girl is performing a routine.'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='A young woman in a black shirt walking down a city street.', questions=['# Would you say these two sentences connect logically? A young lady is fast asleep in her bed.', '# After reading the first sentence, is the second one coherent? A young lady is fast asleep in her bed.', '# Would the following sentence align with what we just read? A young lady is fast asleep in her bed.', '# Looking at the previous context, does this next part hold up? A young lady is fast asleep in her bed.', '# With that setup, does the following sentence work? A young lady is fast asleep in her bed.', '# Based on what was just stated, is this a reasonable follow-up? A young lady is fast asleep in her bed.', '# Following that statement, does this next part flow naturally? A young lady is fast asleep in her bed.', '# Does this next sentence fit with the preceding context? A young lady is fast asleep in her bed.', '# Building on that last sentence, does this next one make sense? A young lady is fast asleep in her bed.', '# Is there logical continuity between these sentences? A young lady is fast asleep in her bed.'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(snli_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
