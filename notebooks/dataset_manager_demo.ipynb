{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import src.dataset_manager as dataset_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geometry_of_truth': ['sp_en_trans',\n",
       "  'cities',\n",
       "  'neg_cities',\n",
       "  'smaller_than',\n",
       "  'larger_than',\n",
       "  'common_claim_true_false',\n",
       "  'companies_true_false'],\n",
       " 'sst2': ['sst2'],\n",
       " 'relations': ['factual/person_plays_instrument',\n",
       "  'factual/person_plays_pro_sport',\n",
       "  'factual/superhero_person',\n",
       "  'factual/country_largest_city',\n",
       "  'factual/food_from_country',\n",
       "  'factual/person_plays_position_in_sport',\n",
       "  'factual/country_capital_city',\n",
       "  'factual/landmark_in_country',\n",
       "  'factual/country_language',\n",
       "  'factual/country_currency',\n",
       "  'factual/superhero_archnemesis',\n",
       "  'factual/city_in_country',\n",
       "  'commonsense/task_done_by_tool',\n",
       "  'commonsense/object_superclass',\n",
       "  'commonsense/fruit_outside_color',\n",
       "  'commonsense/work_location',\n",
       "  'commonsense/substance_phase',\n",
       "  'commonsense/task_done_by_person',\n",
       "  'commonsense/word_sentiment',\n",
       "  'linguistic/adj_antonym',\n",
       "  'linguistic/word_first_letter',\n",
       "  'linguistic/word_last_letter',\n",
       "  'linguistic/adj_comparative',\n",
       "  'linguistic/adj_superlative',\n",
       "  'linguistic/verb_past_tense']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_manager.DatasetManager.list_datasets_by_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry_of_truth_dataset = dataset_manager.DatasetManager.from_dataset_group(\"geometry_of_truth\", batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6478"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geometry_of_truth_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Chizhou is not in China.', questions=['# Is this statement true?', '# Can you confirm the accuracy of this?', '# Do you think this checks out?', '# Am I right to believe this?', '# Are you saying this is correct?', '# Do you think this checks out?', '# Are you confirming this is true?', '# Do the facts support this statement?', '# Would you consider this to be true?', '# Can you confirm the accuracy of this?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='A swan is a species of large waterfowl native to Eurasia and North America.', questions=['# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Would you say this is accurate?', '# Do you agree that this is true?', '# May I assume this is correct?', '# Are you confirming this is true?', '# Has this been verified as true?', '# Is this statement true?', '# Would you say this is accurate?', '# Should I believe this statement?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(geometry_of_truth_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = geometry_of_truth_dataset.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4535 1943\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_dataset = dataset_manager.DatasetManager.from_named_datasets([(\"geometry_of_truth\", \"cities\"), (\"geometry_of_truth\", \"neg_cities\")], batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Korla is in Azerbaijan.', question='Is this true?', answer='No'),\n",
       " ContextQASample(context='The city of Battagram is not in Pakistan.', question='Do you agree that this is true?', answer='No'),\n",
       " ContextQASample(context='The city of San Luis Potosi is not in the Democratic Republic of the Congo.', question='Would you say this is accurate?', answer='Yes'),\n",
       " ContextQASample(context='The city of Phnom Penh is not in Senegal.', question='Could you tell me if this is factual?', answer='Yes')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(named_dataset))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_dataset = dataset_manager.DatasetManager.from_dataset_group(\n",
    "    \"relations\",\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The opposite of give is take.', questions=['# Is this true?', '# Do you find this to be truthful?', '# Would you validate this statement?', '# Can you verify if this is true?', '# Would you consider this to be true?', '# Am I right to believe this?', '# May I assume this is correct?', '# Could you tell me if this is factual?', '# Should I believe this statement?', '# Can we confirm this is accurate?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Jim Bouton plays in the position of a linebacker.', questions=['# Do the facts support this statement?', '# Would you consider this to be true?', '# Is this statement true?', '# Do you agree that this is true?', '# Can you verify if this is true?', '# Could you tell me if this is factual?', '# Is this true?', '# Do you find this to be truthful?', '# Would you say this is accurate?', '# Can we confirm this is accurate?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='Jim Bunning plays the sport of football.', questions=['# May I assume this is correct?', '# Are you saying this is correct?', '# Do you think this checks out?', '# Would you consider this to be true?', '# Am I right to believe this?', '# Can you confirm the accuracy of this?', '# Should I believe this statement?', '# Has this been verified as true?', '# Could you tell me if this is factual?', '# Do you agree that this is true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='West Darfur is in the country of Sudan.', questions=['# Do the facts support this statement?', '# Can you confirm the accuracy of this?', '# Are you confirming this is true?', '# Would you say this is accurate?', '# Would you consider this to be true?', '# Have I understood this correctly?', '# Are you saying this is correct?', '# Do you find this to be truthful?', '# Does this statement hold true?', '# Could you tell me if this is factual?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(relation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_dataset = dataset_manager.DatasetManager.from_dataset_group(\"sst2\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context=\"will at least have a dream image of the west to savor whenever the film 's lamer instincts are in the saddle .\", questions=['# Did the reviewer express dissatisfaction?', '# Is the overall message of this review positive?', '# Reading between the lines, is this a negative review?', '# Does this review indicate a poor experience?', '# Looking at this review, was the person happy with their experience?', '# Looking at this review, was the person unhappy with their experience?', '# Can we consider this a thumbs-up review?', '# Can we consider this a thumbs-down review?', '# Are the comments in this review predominantly positive?', '# Does this review indicate a good experience?'], answers=['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No']),\n",
       " ContextQASample(context='a twisted sense', questions=['# Can we consider this a thumbs-down review?', '# Would you classify this as an endorsement?', '# Can we interpret this as a positive evaluation?', '# Would you say this is an unfavorable review?', '# Is the general sentiment of this review negative?', '# Does this review reflect a positive experience?', '# Does this review lean towards the positive end of the spectrum?', '# Are the comments in this review predominantly positive?', '# Does this review convey customer satisfaction?', '# Would you categorize this as a positive piece of feedback?'], answers=['No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context=\"being lectured to by tech-geeks , if you 're up for that sort of thing\", questions=['# This review is positive. Do you agree?', '# Does this review convey customer satisfaction?', '# Can we consider this a thumbs-up review?', '# Are the comments in this review predominantly negative?', '# Did the reviewer express satisfaction?', '# Would you classify this as a complaint?', '# Does this review express positive sentiment?', '# Did the reviewer express satisfaction?', '# Is the general sentiment of this review positive?', '# Is the general sentiment of this review negative?'], answers=['No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'Yes']),\n",
       " ContextQASample(context=\"a guy dressed as a children 's party clown gets violently gang-raped\", questions=['# Would you say this is an unfavorable review?', '# Are the comments in this review predominantly negative?', '# Can we interpret this as a positive evaluation?', '# Reading between the lines, is this a positive review?', '# Is the overall message of this review positive?', '# Would you classify this as a complaint?', '# Did the reviewer have good things to say?', '# Did the reviewer express satisfaction?', '# Does this review convey customer dissatisfaction?', '# Can we consider this a thumbs-up review?'], answers=['Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(sst2_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dataset = dataset_manager.DatasetManager.from_dataset_group(\"md_gender\", batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Nicole Scherzinger debuted her first solo Killer Lover!\\n\\nThis text is about Killer Love.', questions=['# Does this text feature a male person?', '# Am I reading about a male person here?', '# Is the focus of this sentence on someone female?', '# Does this describe someone who is male?', \"# Is this statement about someone who's female?\", '# Is the subject of this sentence male?', '# Does this portray a male individual?', '# Does this content reference a female person?', '# Am I learning about a male individual here?', '# Is this talking about a female person?'], answers=['No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes']),\n",
       " ContextQASample(context='Frank Crisp who was a 1st Baronet contributed greatly to the Microscopical Society, he sometimes served as officer of the Society.\\n\\nThis text is about Frank Crisp.', questions=['# Does this portray a female individual?', '# Is the character described here female?', '# Is this talking about a female person?', '# Does this passage concern a male person?', '# Is the person mentioned male?', '# Is the subject of this sentence female?', '# Does this refer to a female individual?', '# Is this sentence about a male person?', '# Does this text discuss a male individual?', '# Am I reading about someone who identifies as male?'], answers=['No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Joyce Anne Barr served as an Assistant Secretary of State for Administration and Chief Freedom of Information Act Officers\\n\\nThis text is about Joyce Anne Barr.', questions=['# Is this talking about a male person?', '# Does this describe someone who is female?', '# Is the person mentioned male?', '# Is the subject of this sentence female?', '# Am I reading about someone who identifies as male?', '# Does this text feature a male person?', '# Does this passage concern a male person?', '# Does this refer to a male individual?', '# Is the character described here female?', '# Is the focus of this sentence on someone female?'], answers=['No', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'Yes']),\n",
       " ContextQASample(context='Fatima Manji became the first British TV newsreader to rock a hijab in March 2016\\n\\nThis text is about Fatima Manji.', questions=['# Does this passage concern a male person?', '# Does this text discuss a male individual?', '# Is the character described here female?', '# Am I reading about a male person here?', '# Is this sentence about a male person?', '# Is this narrative about a male individual?', '# Does this content reference a female person?', '# Is the focus of this sentence on someone female?', '# Am I learning about a female individual here?', '# Am I reading about someone who identifies as male?'], answers=['No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(gender_dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
