{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 19:37:12 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import src.utils.logging_utils as logging_utils\n",
    "import src.functional as functional\n",
    "import src.models as models\n",
    "import src.tokens as tokens\n",
    "import src.dataset as dataset\n",
    "from src.models import ModelandTokenizer\n",
    "import src.patchscope_eval as patchscope_eval\n",
    "import proto.patchscope_pb2 as patchscope_pb2\n",
    "import src.dataset_manager as dataset_manager\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 19:37:13 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 19:37:14 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Llama-3.2-3B> | size: 6127.841 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL_KEY = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "MODEL_KEY = \"meta-llama/Llama-3.2-3B\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "# MODEL_KEY = \"google/gemma-2-9b-it\"\n",
    "# MODEL_KEY = \"google/gemma-2-2b\"\n",
    "\n",
    "mt = models.ModelandTokenizer(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "mt.n_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_prompt = \"The city of Paris is in the country of France.\"\n",
    "# false_prompt = \"The city of Paris is in the country of Italy.\"\n",
    "# true_input, false_input = [tokens.prepare_input(p, mt) for p in (true_prompt, false_prompt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # layers = [24]\n",
    "# layers = list(range(3, 30))\n",
    "\n",
    "# true_h, false_h = [\n",
    "#     patchscope_eval.get_h_layers(mt, input_, layers) for input_ in (true_input, false_input)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_token, false_token = [mt.tokenizer.encode(t)[-1] for t in (\"True\", \"False\")]\n",
    "# [mt.tokenizer.decode(t) for t in (true_token, false_token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for prompt, h in zip((true_prompt, false_prompt), (true_h, false_h)):\n",
    "#     print(prompt)\n",
    "#     _, result_dict = functional.patchscope(\n",
    "#         mt = mt, \n",
    "#         hs = patchscope_eval.get_h_with_target_layer(h, 3) if len(h) == 1 else h,\n",
    "#         target_prompt = target_prompt,\n",
    "#         interested_tokens = (true_token, false_token),\n",
    "#         k = 5)\n",
    "#     for t in (true_token, false_token):\n",
    "#         print(\"   \", result_dict[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmt_dataset = dataset.GMTDataset.from_csv(\"cities.csv\", few_shot=False).examples\n",
    "# len(gmt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = '''The city of Tokyo is in Japan. This statement is: True\n",
    "The city of Hanoi is in Poland. This statement is: False\n",
    "placeholder placeholder placeholder placeholder placeholder This statement is: '''\n",
    "\n",
    "instruct_prompt = '''<|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "True or false: placeholder placeholder placeholder placeholder placeholder<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "'''\n",
    "\n",
    "target_prompt = {\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\" : instruct_prompt,\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\" : instruct_prompt,\n",
    "    \"meta-llama/Llama-3.2-3B\" : base_prompt,\n",
    "    \"meta-llama/Llama-3.1-8B\" : base_prompt,\n",
    "}[MODEL_KEY]\n",
    "\n",
    "model_short_name = {\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\" : \"llama_3b_instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\" : \"llama_8b_instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B\" : \"llama_3b\",\n",
    "    \"meta-llama/Llama-3.1-8B\" : \"llama_8b\",\n",
    "}[MODEL_KEY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.1-8B llama_8b\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_KEY, model_short_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:37<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sp_en_trans.csv 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:36<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_sp_en_trans.csv 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:36<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cities.csv 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:36<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_cities.csv 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:36<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smaller_than.csv 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:34<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "larger_than.csv 0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_claim_true_false.csv 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "companies_true_false.csv 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:32<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterfact_true_false.csv 0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result_set_name = model_short_name + \"__all_layers\"\n",
    "evaluation_results = patchscope_pb2.EvaluationResults()\n",
    "\n",
    "for filename in dataset.GMT_DATA_FILES:\n",
    "    examples = dataset.GMTDataset.simple_get_examples(filename)[:100]\n",
    "    evaluation_config = patchscope_pb2.EvaluationConfig(\n",
    "        model_key=MODEL_KEY,\n",
    "        dataset=filename,\n",
    "        target_prompt=target_prompt,\n",
    "        label_to_token={ \"1\": \"True\", \"0\": \"False\" },\n",
    "        patchscope_config=patchscope_pb2.PatchscopeConfig(\n",
    "            source_layers=list(range(mt.n_layer)),\n",
    "            target_layers=[]\n",
    "        )\n",
    "    )\n",
    "    evaluation_runner = patchscope_eval.EvaluationRunner(evaluation_config, mt)\n",
    "    evaluation_result = evaluation_runner.evaluate(examples)\n",
    "    evaluation_result.result_set_name = result_set_name\n",
    "    # evaluation_result = evaluation_runner.evaluate([(true_prompt, True), (false_prompt, False)])\n",
    "    evaluation_results.results.append(evaluation_result)\n",
    "    print(filename, evaluation_result.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results/{result_set_name}.binpb\", \"wb\") as f:\n",
    "    f.write(evaluation_results.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuned evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 19:37:23 src.models INFO     loaded model <EleutherAI/gpt-j-6B> | size: 6127.841 MB | dtype: torch.bfloat16 | device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# finetuned_model_key = \"sensharma/Llama-3.2-3B_Talkative_Probe_Step-3000\"\n",
    "finetuned_model_key = \"/home/local_arnab/Codes/Projects/talkative_probes/results/patchscope_save/Llama-3.2-3B/checkpoint-12000\"\n",
    "tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_model_key, torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "tuned_lm = LanguageModel(tuned_model)\n",
    "decoder_mt = ModelandTokenizer(\n",
    "    base_lm = tuned_lm,\n",
    "    tokenizer = mt.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_mt.name = \"Llama-3.2-3B/checkpoint-12000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_group = \"geometry_of_truth\"\n",
    "(dataset,) = dataset_manager.DatasetManager.from_dataset_group(dataset_group).split([0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='Peasants originated in Great Britain and were later introduced in Europe.', questions=['# Do you find this to be truthful?', '# Would you validate this statement?', '# Have I understood this correctly?', '# Do you agree that this is true?', '# May I assume this is correct?', '# Can you confirm the accuracy of this?', '# Is this statement true?', '# Are you confirming this is true?', '# Could you tell me if this is factual?', '# Do the facts support this statement?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    791,   3363,    315,  12366,    374,    304,    279,   3224,\n",
       "            315]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'offset_mapping': tensor([[[ 0,  0],\n",
       "         [ 0,  3],\n",
       "         [ 3,  8],\n",
       "         [ 8, 11],\n",
       "         [11, 17],\n",
       "         [17, 20],\n",
       "         [20, 23],\n",
       "         [23, 27],\n",
       "         [27, 35],\n",
       "         [35, 38]]], device='cuda:0')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tokens import prepare_input, find_token_range\n",
    "\n",
    "prompts = [\n",
    "    # \"The land of\",\n",
    "    # \"The capital of France is\",\n",
    "    # \"This is a\"\n",
    "    \"The city of Paris is in the country of\"\n",
    "]\n",
    "\n",
    "batch_inputs = prepare_input(\n",
    "    tokenizer=mt,\n",
    "    prompts=prompts,\n",
    "    padding_side=\"left\",\n",
    "    # padding=\"max_length\",\n",
    "    # max_length=20,\n",
    "    truncation=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130/130 [00:50<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6692307692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_config = patchscope_pb2.EvaluationConfig(\n",
    "    model_key=MODEL_KEY,\n",
    "    dataset=dataset_group,\n",
    "    patchscope_config=patchscope_pb2.PatchscopeConfig(\n",
    "        source_layers=[24],\n",
    "        target_layers=list(range(decoder_mt.n_layer))\n",
    "    ),\n",
    "    decoder_config=patchscope_pb2.DecoderConfig(\n",
    "        name=finetuned_model_key\n",
    "    )\n",
    ")\n",
    "evaluation_runner = patchscope_eval.EvaluationRunner(evaluation_config, mt, decoder_mt)\n",
    "evaluation_result = evaluation_runner.evaluate(dataset)\n",
    "print(evaluation_result.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[PredictedToken(token=' Yes', prob=0.5310860276222229, logit=21.25, token_id=7566),\n",
       "  PredictedToken(token=' No', prob=0.46868178248405457, logit=21.125, token_id=2360),\n",
       "  PredictedToken(token='Yes', prob=6.976826989557594e-05, logit=12.3125, token_id=9642),\n",
       "  PredictedToken(token=' yes', prob=4.79509835713543e-05, logit=11.9375, token_id=10035),\n",
       "  PredictedToken(token=' YES', prob=2.7321648303768598e-05, logit=11.375, token_id=14410),\n",
       "  PredictedToken(token='No', prob=7.353521596087376e-06, logit=10.0625, token_id=2822),\n",
       "  PredictedToken(token=' Yeah', prob=6.489459792646812e-06, logit=9.9375, token_id=22335),\n",
       "  PredictedToken(token='.No', prob=3.6975827697460772e-06, logit=9.375, token_id=17184),\n",
       "  PredictedToken(token='yes', prob=3.2631053272780264e-06, logit=9.25, token_id=9891),\n",
       "  PredictedToken(token='-No', prob=2.87968032353092e-06, logit=9.125, token_id=99076)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt = \"The boys are playing in the park. This statement is in the present tense.\"\n",
    "prompt = \"Grace is a school teacher. Would you say this statement is about a male?\"\n",
    "\n",
    "functional.predict_next_token(decoder_mt, prompt, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The city of Paris is in the country of France Yes Yes Yes Yes Yes Yes Yes Yes Yes'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = prepare_input(\n",
    "    tokenizer=decoder_mt,\n",
    "    prompts=[prompt],\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "output = decoder_mt._model.generate(\n",
    "    **inputs, do_sample = False\n",
    ")\n",
    "\n",
    "mt.tokenizer.decode(output[0], skip_special_tokens=True)  # Decode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.functional import get_batch_concept_activations\n",
    "\n",
    "prompts = [\n",
    "    \"Grace is a school teacher\",\n",
    "]\n",
    "\n",
    "latents = get_batch_concept_activations(\n",
    "    mt=mt,\n",
    "    prompts=prompts,\n",
    "    interested_layer_indices=list(range(5, 20)),\n",
    "    check_prediction=None,\n",
    "    on_token_occur=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents[0].questions = [\"# Would you say this statement is about a male?\"]\n",
    "latents[0].answers = [\" No\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation_manager import ActivationSample\n",
    "import random\n",
    "\n",
    "add_to_buffer = []\n",
    "for latent_cache in latents:\n",
    "    for layer_name in latent_cache.latents.keys():\n",
    "        activation = latent_cache.latents[layer_name]\n",
    "        # query, label = self.get_latent_qa(\n",
    "        #     correct_label=latent_cache.correct_label,\n",
    "        #     wrong_label=latent_cache.incorrect_label,\n",
    "        #     group=latent_cache.group,\n",
    "        # )\n",
    "        question, label = random.choice(\n",
    "            list(zip(latent_cache.questions, latent_cache.answers))\n",
    "        )\n",
    "        add_to_buffer.append(\n",
    "            ActivationSample(\n",
    "                activation=activation,\n",
    "                context=latent_cache.context,\n",
    "                question=question,\n",
    "                label=label,\n",
    "                layer_name=layer_name,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/nnsight/tracing/Node.py:380: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = self.target(*args, **kwargs)\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.train_utils import evaluate\n",
    "\n",
    "evaluate(decoder_mt, add_to_buffer)  # Evaluate the collected sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for facebook/md_gender_bias contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/facebook/md_gender_bias\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ds = dataset_manager.DatasetManager.from_named_datasets(\n",
    "    [(\"md_gender\", \"md_gender\")],\n",
    "    batch_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': \"I'm a football fan that knows he plays for FC Tamboc from Arsenal.\\n\\nThis text is about Vladislav Ryzhkov.\",\n",
       " 'questions': ['# Am I reading about someone who identifies as female?',\n",
       "  '# Is the person mentioned male?',\n",
       "  '# Does this portray a male individual?',\n",
       "  '# Is the subject of this sentence male?',\n",
       "  '# Does this content reference a male person?',\n",
       "  '# Is the character described here female?',\n",
       "  '# Am I learning about a male individual here?',\n",
       "  '# Is this about someone who is male?',\n",
       "  '# Is the focus of this sentence on someone male?',\n",
       "  '# Does this refer to a female individual?'],\n",
       " 'answers': ['No',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'No',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'Yes',\n",
       "  'No']}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
