{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-28 15:25:53 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../chess_llm_interpretability\")\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils, experiment_utils\n",
    "from src import functional\n",
    "import wandb\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-28 15:25:54 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-28 15:25:58 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Llama-3.2-3B> | size: 12255.675 MB | dtype: torch.float32 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "MODEL_KEY = \"meta-llama/Llama-3.2-3B\"\n",
    "# MODEL_KEY = \"google/gemma-2-2b\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#! torch.adaptive precision\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from src.utils.typing import LatentCache, LatentCacheCollection\n",
    "from src.tokens import prepare_input\n",
    "from src.functional import get_module_nnsight, interpret_logits\n",
    "from src.functional import get_batch_concept_activations\n",
    "\n",
    "# prompts = [\n",
    "#     \"Eiffel Tower is in which city? It is in\",\n",
    "#     \"A quick brown fox\",\n",
    "#     \"The sun rises in the\",\n",
    "# ]\n",
    "\n",
    "prompts = [\n",
    "    \"The land of the rising sun is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The Space Needle is a tower in\"\n",
    "]\n",
    "\n",
    "latents = get_batch_concept_activations(\n",
    "    mt=mt,\n",
    "    prompts=prompts,\n",
    "    interested_layer_indices=list(range(5, 20)),\n",
    "    check_prediction=None,\n",
    "    on_token_occur=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcc = LatentCacheCollection(latents=latents)\n",
    "lcc.detensorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"lcc_batch/test_lcc.json\", \"w\") as f:\n",
    "    f.write(lcc.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-28 16:52:38 src.dataset_manager INFO     Loaded 60 examples from commonsense/fruit_outside_color.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "# group, name = \"relations\", \"factual/country_capital_city\"\n",
    "group, name = \"relations\", \"commonsense/fruit_outside_color\"\n",
    "\n",
    "dataloader = DatasetManager.from_named_datasets(\n",
    "    [\n",
    "        (group, name),\n",
    "    ],\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:04,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "cache_dir = os.path.join(group, name)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "    prompts = [b.context for b in batch]    \n",
    "    latents = get_batch_concept_activations(\n",
    "        mt=mt,\n",
    "        prompts=prompts,\n",
    "        interested_layer_indices=list(range(5, 20)),\n",
    "        check_prediction=None,\n",
    "        on_token_occur=None,\n",
    "    )\n",
    "\n",
    "    correct_labels = [b.correct for b in batch]\n",
    "    incorrect_labels = [b.incorrect for b in batch]\n",
    "\n",
    "    for latent_cache, correct, incorrect in zip(latents, correct_labels, incorrect_labels):\n",
    "        latent_cache.correct_label = correct\n",
    "        latent_cache.incorrect_label = incorrect\n",
    "        latent_cache.group=\"relations\"\n",
    "    \n",
    "    lcc = LatentCacheCollection(latents=latents)\n",
    "    lcc.detensorize()\n",
    "\n",
    "    with open(os.path.join(cache_dir, f\"batch_{batch_idx}.json\"), \"w\") as f:\n",
    "        f.write(lcc.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from src.utils.typing import ArrayLike\n",
    "from typing import Literal\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class ActivationSample:\n",
    "    activation: ArrayLike\n",
    "    context: str\n",
    "    query: str\n",
    "    label: Literal[\"yes\", \"no\"]\n",
    "    layer_name: str | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.activation, torch.Tensor) == False:\n",
    "            self.activation = torch.Tensor(self.activation)\n",
    "        \n",
    "        assert self.label in [\"yes\", \"no\"]\n",
    "        assert \"#\" in self.query\n",
    "    \n",
    "\n",
    "class ActivationLoader:\n",
    "    def __init__(self, latent_cache_files: str, shuffle: bool = True, batch_size: int = 32):\n",
    "        self.latent_cache_files = []\n",
    "        for file_path in latent_cache_files:\n",
    "            if os.path.exists(file_path) == False:\n",
    "                logger.error(f\"{file_path} not found\")\n",
    "                continue    \n",
    "            if os.path.isdir(file_path) == True:\n",
    "                raise logger.error(f\"{file_path} should be a json file\")\n",
    "            self.latent_cache_files.append(file_path)\n",
    "        \n",
    "        if shuffle:\n",
    "            random.shuffle(self.latent_cache_files)\n",
    "\n",
    "        self.current_file_idx = 0\n",
    "        self.buffer: list[ActivationSample] = []\n",
    "        self.batch_size = batch_size\n",
    "        self.stop_iteration = False\n",
    "\n",
    "        with open(\n",
    "            os.path.join(env_utils.DEFAULT_DATA_DIR, \"paraphrases/yes_no.json\"), \"r\"\n",
    "        ) as f:\n",
    "            self.YES_NO_PARAPHRASES = json.load(f)\n",
    "\n",
    "        with open(\n",
    "            os.path.join(env_utils.DEFAULT_DATA_DIR, \"paraphrases/question.json\"), \"r\"\n",
    "        ) as f:\n",
    "            self.QUESTION_PARAPHRASES = json.load(f)\n",
    "    \n",
    "    def get_latent_qa(self, correct_label, wrong_label, group) -> tuple[str, Literal[\"yes\", \"no\"]]:\n",
    "        label = random.choice([\"yes\", \"no\"])\n",
    "        yes_no = random.choice(self.YES_NO_PARAPHRASES)\n",
    "        question = random.choice(self.QUESTION_PARAPHRASES[group])\n",
    "        \n",
    "        query = \"# \"\n",
    "        question = question.format(correct_label) if label == \"yes\" else question.format(wrong_label)\n",
    "        query += question + f\" {yes_no}\"\n",
    "        \n",
    "        return query, label\n",
    "\n",
    "\n",
    "    def load_next_file(self):\n",
    "        if self.current_file_idx >= len(self.latent_cache_files):\n",
    "            return False\n",
    "\n",
    "        with open(self.latent_cache_files[self.current_file_idx], \"r\") as f:\n",
    "            lcc = LatentCacheCollection.from_json(f.read())\n",
    "        \n",
    "        add_to_buffer = []\n",
    "        for latent_cache in lcc.latents:\n",
    "            for layer_name in latent_cache.latents.keys():\n",
    "                activation = latent_cache.latents[layer_name]\n",
    "                query, label = self.get_latent_qa(\n",
    "                    correct_label=latent_cache.correct_label,\n",
    "                    wrong_label=latent_cache.incorrect_label,\n",
    "                    group=latent_cache.group,\n",
    "                )\n",
    "                add_to_buffer.append(ActivationSample(\n",
    "                    activation=activation,\n",
    "                    context=latent_cache.context,\n",
    "                    query=query,\n",
    "                    label=label,\n",
    "                    layer_name=layer_name,\n",
    "                ))\n",
    "        \n",
    "        random.shuffle(add_to_buffer)\n",
    "        self.buffer.extend(add_to_buffer)\n",
    "        self.current_file_idx += 1\n",
    "        return True\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self.stop_iteration:\n",
    "            raise StopIteration\n",
    "        \n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            if self.load_next_file() == False:\n",
    "                self.stop_iteration = True      # will raise StopIteration next time\n",
    "\n",
    "        batch = self.buffer[:self.batch_size]\n",
    "        self.buffer = self.buffer[self.batch_size:]\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relations/commonsense/fruit_outside_color/batch_6.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_0.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_1.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_5.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_2.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_4.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_7.json',\n",
       " 'relations/commonsense/fruit_outside_color/batch_3.json',\n",
       " 'relations/factual/country_capital_city/batch_0.json',\n",
       " 'relations/factual/country_capital_city/batch_1.json',\n",
       " 'relations/factual/country_capital_city/batch_5.json',\n",
       " 'relations/factual/country_capital_city/batch_2.json',\n",
       " 'relations/factual/country_capital_city/batch_4.json',\n",
       " 'relations/factual/country_capital_city/batch_3.json']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_root = \"relations\"\n",
    "\n",
    "def get_batch_paths(root):\n",
    "    for root, _, files in os.walk(root):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                yield os.path.join(root, file)\n",
    "\n",
    "\n",
    "batch_paths = list(get_batch_paths(relation_root))\n",
    "batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_loader = ActivationLoader(\n",
    "    latent_cache_files=batch_paths,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_batch = act_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geometry_of_truth': ['sp_en_trans',\n",
       "  'neg_sp_en_trans',\n",
       "  'cities',\n",
       "  'neg_cities',\n",
       "  'smaller_than',\n",
       "  'larger_than',\n",
       "  'common_claim_true_false',\n",
       "  'companies_true_false',\n",
       "  'counterfact_true_false'],\n",
       " 'sst2': ['sst2'],\n",
       " 'relations': ['commonsense/word_sentiment',\n",
       "  'commonsense/fruit_outside_color',\n",
       "  'commonsense/task_done_by_person',\n",
       "  'commonsense/work_location',\n",
       "  'commonsense/task_done_by_tool',\n",
       "  'commonsense/substance_phase',\n",
       "  'commonsense/object_superclass',\n",
       "  'commonsense/fruit_inside_color',\n",
       "  'factual/pokemon_evolutions',\n",
       "  'factual/country_capital_city',\n",
       "  'factual/person_plays_pro_sport',\n",
       "  'factual/star_constellation',\n",
       "  'factual/country_language',\n",
       "  'factual/presidents_birth_year',\n",
       "  'factual/landmark_on_continent',\n",
       "  'factual/country_largest_city',\n",
       "  'factual/company_hq',\n",
       "  'factual/food_from_country',\n",
       "  'factual/landmark_in_country',\n",
       "  'factual/company_ceo',\n",
       "  'factual/superhero_archnemesis',\n",
       "  'factual/city_in_country',\n",
       "  'factual/person_band_lead_singer',\n",
       "  'factual/person_father',\n",
       "  'factual/person_mother',\n",
       "  'factual/superhero_person',\n",
       "  'factual/person_occupation',\n",
       "  'factual/presidents_election_year',\n",
       "  'factual/person_plays_instrument',\n",
       "  'factual/country_currency',\n",
       "  'factual/product_by_company',\n",
       "  'factual/person_university',\n",
       "  'factual/person_plays_position_in_sport',\n",
       "  'factual/person_native_language',\n",
       "  'linguistic/word_last_letter',\n",
       "  'linguistic/verb_past_tense',\n",
       "  'linguistic/word_first_letter',\n",
       "  'linguistic/adj_superlative',\n",
       "  'linguistic/adj_comparative',\n",
       "  'linguistic/adj_antonym']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetManager.list_datasets_by_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
