{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/local_arnab/miniconda3/envs/retrieval/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:30:10 __main__ INFO     torch.__version__='2.5.0+cu124', torch.version.cuda='12.4'\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils, experiment_utils\n",
    "from src import functional\n",
    "import wandb\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.46.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:30:10 accelerate.utils.modeling INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:30:14 src.models INFO     loaded model </home/local_arnab/Codes/00_MODEL/meta-llama/Llama-3.2-3B> | size: 12255.675 MB | dtype: torch.float32 | device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "MODEL_KEY = \"meta-llama/Llama-3.2-3B\"\n",
    "# MODEL_KEY = \"google/gemma-2-2b\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#! torch.adaptive precision\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=MODEL_KEY,\n",
    "    torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 19:10:27 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-10-30 19:10:27 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n",
      "2024-10-30 19:10:27 datasets INFO     PyTorch version 2.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_manager import DatasetManager\n",
    "dataloader = DatasetManager.from_named_datasets(\n",
    "    [(\"geometry_of_truth\", \"cities\")],\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='The city of Zhangjiagang is in China.', questions=['# Are these facts correct?', '# Are you saying this is correct?', '# Do you agree that this is true?', '# Could you tell me if this is factual?', '# Has this been verified as true?', '# Have I understood this correctly?', '# Would you validate this statement?', '# Do you find this to be truthful?', '# Can you confirm the accuracy of this?', '# Would you consider this to be true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Antananarivo is in Germany.', questions=['# Is this statement true?', '# Am I right to believe this?', '# Could you tell me if this is factual?', '# Do the facts support this statement?', '# Would you consider this to be true?', '# Do you think this checks out?', '# Does this statement hold true?', '# Should I believe this statement?', '# Would you validate this statement?', '# Are these facts correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Jilin is in China.', questions=['# Should I believe this statement?', '# Do you agree that this is true?', '# Is this true?', '# Does this information ring true?', '# Are you confirming this is true?', '# Is this statement true?', '# Can you confirm the accuracy of this?', '# Would you consider this to be true?', '# Would you say this is accurate?', '# Do you think this checks out?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Ebute Ikorodu is in China.', questions=['# Are you saying this is correct?', '# Should I believe this statement?', '# Do you agree that this is true?', '# Would you say this is accurate?', '# Does this information ring true?', '# Is this true?', '# Does this statement hold true?', '# Has this been verified as true?', '# Do the facts support this statement?', '# Do you find this to be truthful?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Jeonju is in South Korea.', questions=['# Are you confirming this is true?', '# Am I right to believe this?', '# Would you say this is accurate?', '# Have I understood this correctly?', '# Are these facts correct?', '# Has this been verified as true?', '# Do you think this checks out?', '# May I assume this is correct?', '# Would you validate this statement?', '# Can we confirm this is accurate?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Huaihua is in South Africa.', questions=['# Are these facts correct?', '# Does this statement hold true?', '# Do you think this checks out?', '# Am I right to believe this?', '# Are you confirming this is true?', '# Have I understood this correctly?', '# Would you validate this statement?', '# Has this been verified as true?', '# Does this information ring true?', '# Do you find this to be truthful?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Changde is in China.', questions=['# Are you confirming this is true?', '# Does this statement hold true?', '# Do you agree that this is true?', '# Does this information ring true?', '# Am I right to believe this?', '# Would you consider this to be true?', '# Can you verify if this is true?', '# Would you say this is accurate?', '# Can you confirm the accuracy of this?', '# Do the facts support this statement?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Porto Alegre is in Belarus.', questions=['# Do the facts support this statement?', '# Does this statement hold true?', '# Would you say this is accurate?', '# Can we confirm this is accurate?', '# Are these facts correct?', '# Do you think this checks out?', '# Do you find this to be truthful?', '# Has this been verified as true?', '# Am I right to believe this?', '# May I assume this is correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Kolkata is in India.', questions=['# Does this statement hold true?', '# Are these facts correct?', '# Can we confirm this is accurate?', '# Can you confirm the accuracy of this?', '# Has this been verified as true?', '# Would you validate this statement?', '# Do you agree that this is true?', '# Should I believe this statement?', '# May I assume this is correct?', '# Does this information ring true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Konya is in Turkey.', questions=['# Do the facts support this statement?', '# Would you validate this statement?', '# Are these facts correct?', '# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Do you agree that this is true?', '# Could you tell me if this is factual?', '# Would you say this is accurate?', '# Is this statement true?', '# Is this true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Sharjah is in the United Arab Emirates.', questions=['# Would you consider this to be true?', '# Can you verify if this is true?', '# Do the facts support this statement?', '# Do you think this checks out?', '# Are you confirming this is true?', '# Should I believe this statement?', '# Does this statement hold true?', '# May I assume this is correct?', '# Could you tell me if this is factual?', '# Does this information ring true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Tangerang is in Indonesia.', questions=['# Has this been verified as true?', '# Do you think this checks out?', '# Are you saying this is correct?', '# Would you consider this to be true?', '# Can we confirm this is accurate?', '# Can you verify if this is true?', '# Do the facts support this statement?', '# Does this information ring true?', '# May I assume this is correct?', '# Have I understood this correctly?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Makati City is in India.', questions=['# Is this statement true?', '# Can you verify if this is true?', '# Has this been verified as true?', '# Should I believe this statement?', '# Are these facts correct?', '# Does this statement hold true?', '# Do you agree that this is true?', '# Can you confirm the accuracy of this?', '# May I assume this is correct?', '# Do the facts support this statement?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Battagram is in Tanzania.', questions=['# Could you tell me if this is factual?', '# Has this been verified as true?', '# Is this statement true?', '# Would you say this is accurate?', '# Would you validate this statement?', '# Does this information ring true?', '# May I assume this is correct?', '# Have I understood this correctly?', '# Do you agree that this is true?', '# Are you saying this is correct?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Ibague is in Colombia.', questions=['# Could you tell me if this is factual?', '# Has this been verified as true?', '# Does this statement hold true?', '# Is this true?', '# Would you validate this statement?', '# Would you say this is accurate?', '# Am I right to believe this?', '# Do the facts support this statement?', '# Does this information ring true?', '# Are you confirming this is true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Sao Bernardo do Campo is in Brazil.', questions=['# Do the facts support this statement?', '# Do you agree that this is true?', '# Would you consider this to be true?', '# Can we confirm this is accurate?', '# Do you think this checks out?', '# Can you verify if this is true?', '# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Would you say this is accurate?', '# Could you tell me if this is factual?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Mogadishu is in Somalia.', questions=['# Do you find this to be truthful?', '# Have I understood this correctly?', '# Does this information ring true?', '# Would you validate this statement?', '# Could you tell me if this is factual?', '# Has this been verified as true?', '# Can you confirm the accuracy of this?', '# Is this true?', '# Can we confirm this is accurate?', '# Is this statement true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Subang Jaya is in Germany.', questions=['# Am I right to believe this?', '# Would you consider this to be true?', '# Are these facts correct?', '# Can you verify if this is true?', '# Do you agree that this is true?', '# Would you say this is accurate?', '# Has this been verified as true?', '# Do you find this to be truthful?', '# Could you tell me if this is factual?', '# Do the facts support this statement?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Uberlandia is in India.', questions=['# Do you think this checks out?', '# Are you confirming this is true?', '# Has this been verified as true?', '# Am I right to believe this?', '# Can you verify if this is true?', '# Does this information ring true?', '# Are you saying this is correct?', '# Can we confirm this is accurate?', '# Are these facts correct?', '# Do you agree that this is true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Khabarovsk Vtoroy is in Russia.', questions=['# May I assume this is correct?', '# Does this information ring true?', '# Can you verify if this is true?', '# Do you agree that this is true?', '# Can you confirm the accuracy of this?', '# Do you find this to be truthful?', '# Do the facts support this statement?', '# Is this true?', '# Is this statement true?', '# Are these facts correct?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Jaboatao is in Brazil.', questions=['# Are you saying this is correct?', '# Have I understood this correctly?', '# Is this statement true?', '# Do you find this to be truthful?', '# Would you validate this statement?', '# Does this statement hold true?', '# Should I believe this statement?', '# Can you confirm the accuracy of this?', '# Do you agree that this is true?', '# Would you say this is accurate?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Xuchang is in Japan.', questions=['# Are these facts correct?', '# Can we confirm this is accurate?', '# Should I believe this statement?', '# Do you find this to be truthful?', '# Could you tell me if this is factual?', '# Is this true?', '# Do you think this checks out?', '# Am I right to believe this?', '# Have I understood this correctly?', '# Can you confirm the accuracy of this?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Hohhot is in Indonesia.', questions=['# Do you think this checks out?', '# Do you find this to be truthful?', '# Would you consider this to be true?', '# Do you agree that this is true?', '# Should I believe this statement?', '# Would you say this is accurate?', '# Does this information ring true?', '# Can you confirm the accuracy of this?', '# Has this been verified as true?', '# Have I understood this correctly?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Wroclaw is in Poland.', questions=['# Can we confirm this is accurate?', '# Could you tell me if this is factual?', '# Are you confirming this is true?', '# Would you consider this to be true?', '# May I assume this is correct?', '# Am I right to believe this?', '# Are these facts correct?', '# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Have I understood this correctly?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Zahedan is in Nicaragua.', questions=['# Can you confirm the accuracy of this?', '# May I assume this is correct?', '# Could you tell me if this is factual?', '# Am I right to believe this?', '# Have I understood this correctly?', '# Do the facts support this statement?', '# Would you validate this statement?', '# Can you verify if this is true?', '# Do you find this to be truthful?', '# Do you agree that this is true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Qom is in Iran.', questions=['# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Has this been verified as true?', '# Do you think this checks out?', '# Are these facts correct?', '# Do you agree that this is true?', '# Would you validate this statement?', '# Is this true?', '# Are you confirming this is true?', '# Am I right to believe this?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Ningbo is in South Korea.', questions=['# Does this statement hold true?', '# Would you validate this statement?', '# Would you consider this to be true?', '# Is this true?', '# Do the facts support this statement?', '# Can you confirm the accuracy of this?', '# Does this information ring true?', '# May I assume this is correct?', '# Do you find this to be truthful?', '# Has this been verified as true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Linfen is in China.', questions=['# Have I understood this correctly?', '# Should I believe this statement?', '# Do you find this to be truthful?', '# Can you confirm the accuracy of this?', '# Would you consider this to be true?', '# May I assume this is correct?', '# Could you tell me if this is factual?', '# Do you think this checks out?', '# Is this true?', '# Has this been verified as true?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes']),\n",
       " ContextQASample(context='The city of Aracaju is in Russia.', questions=['# Should I believe this statement?', '# Does this statement hold true?', '# Can you confirm the accuracy of this?', '# Are you saying this is correct?', '# Am I right to believe this?', '# Is this statement true?', '# Do the facts support this statement?', '# Could you tell me if this is factual?', '# Has this been verified as true?', '# Do you agree that this is true?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Asuncion is in Nigeria.', questions=['# Does this information ring true?', '# Is this true?', '# Am I right to believe this?', '# Is this statement true?', '# Has this been verified as true?', '# Can you verify if this is true?', '# Would you consider this to be true?', '# Are you confirming this is true?', '# Are you saying this is correct?', '# Should I believe this statement?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Kakamega is in Russia.', questions=['# Does this statement hold true?', '# Is this statement true?', '# Would you consider this to be true?', '# Has this been verified as true?', '# Are these facts correct?', '# Do you think this checks out?', '# Does this information ring true?', '# Should I believe this statement?', '# Do you find this to be truthful?', '# Could you tell me if this is factual?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No']),\n",
       " ContextQASample(context='The city of Sorocaba is in Mexico.', questions=['# Does this statement hold true?', '# Are these facts correct?', '# Are you saying this is correct?', '# Can we confirm this is accurate?', '# Are you confirming this is true?', '# Does this information ring true?', '# Is this true?', '# Can you verify if this is true?', '# Do you agree that this is true?', '# Have I understood this correctly?'], answers=['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from src.utils.typing import LatentCache, LatentCacheCollection\n",
    "from src.tokens import prepare_input\n",
    "from src.functional import get_module_nnsight, interpret_logits\n",
    "from src.functional import get_batch_concept_activations\n",
    "\n",
    "# prompts = [\n",
    "#     \"Eiffel Tower is in which city? It is in\",\n",
    "#     \"A quick brown fox\",\n",
    "#     \"The sun rises in the\",\n",
    "# ]\n",
    "\n",
    "prompts = [\n",
    "    \"The land of the rising sun is\",\n",
    "    \"The capital of France is\",\n",
    "    \"The Space Needle is a tower in\"\n",
    "]\n",
    "\n",
    "latents = get_batch_concept_activations(\n",
    "    mt=mt,\n",
    "    prompts=prompts,\n",
    "    interested_layer_indices=list(range(5, 20)),\n",
    "    check_prediction=None,\n",
    "    on_token_occur=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1620,  0.0499, -0.3556,  ..., -0.1090, -0.0160,  0.0129],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act = latents[0].latents[mt.layer_name_format.format(10)]\n",
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1620,  0.0499, -0.3556,  ..., -0.1090, -0.0160,  0.0129],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.functional import get_module_nnsight\n",
    "with mt.trace(prompts[0]):\n",
    "    module = get_module_nnsight(mt, mt.layer_name_format.format(10))\n",
    "    act2 = module.output[0][0, -1, :].save()\n",
    "\n",
    "act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(act, act2, atol=1e-5)  # Check if the two activations are close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.activation_manager import get_batch_paths, ActivationLoader\n",
    "\n",
    "batch_paths = list(get_batch_paths(\"/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense\"))\n",
    "\n",
    "loader = ActivationLoader(batch_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_utils import prepare_batch_input\n",
    "\n",
    "batch_inputs, int_tok = prepare_batch_input(batch, mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 #\n",
      "5 #\n",
      "4 #\n",
      "5 #\n",
      "1 #\n",
      "5 #\n",
      "3 #\n",
      "4 #\n",
      "2 #\n",
      "4 #\n",
      "5 #\n",
      "1 #\n",
      "1 #\n",
      "4 #\n",
      "1 #\n",
      "4 #\n",
      "5 #\n",
      "4 #\n",
      "1 #\n",
      "3 #\n",
      "1 #\n",
      "4 #\n",
      "4 #\n",
      "5 #\n",
      "5 #\n",
      "4 #\n",
      "4 #\n",
      "2 #\n",
      "5 #\n",
      "1 #\n",
      "1 #\n",
      "2 #\n"
     ]
    }
   ],
   "source": [
    "for inp, tok in zip(batch_inputs.input_ids, int_tok):\n",
    "    # print(inp, tok)\n",
    "    print(tok, mt.tokenizer.decode([inp[tok]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcc = LatentCacheCollection(latents=latents)\n",
    "lcc.detensorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-31 11:30:33 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-10-31 11:30:33 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n",
      "2024-10-31 11:30:33 datasets INFO     PyTorch version 2.5.0 available.\n",
      "2024-10-31 11:30:33 src.dataset_manager INFO     Loaded 60 examples from commonsense/fruit_outside_color.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset_manager import DatasetManager\n",
    "\n",
    "# group, name = \"relations\", \"factual/country_capital_city\"\n",
    "group, name = \"relations\", \"commonsense/fruit_outside_color\"\n",
    "\n",
    "dataloader = DatasetManager.from_named_datasets(\n",
    "    [\n",
    "        (group, name),\n",
    "    ],\n",
    "    batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mgroup\u001b[49m, name)\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(cache_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(dataloader)):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'group' is not defined"
     ]
    }
   ],
   "source": [
    "cache_dir = os.path.join(group, name)\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "for batch_idx, batch in tqdm(enumerate(dataloader)):\n",
    "    prompts = [b.context for b in batch]    \n",
    "    latents = get_batch_concept_activations(\n",
    "        mt=mt,\n",
    "        prompts=prompts,\n",
    "        interested_layer_indices=list(range(5, 20)),\n",
    "        check_prediction=None,\n",
    "        on_token_occur=None,\n",
    "    )\n",
    "\n",
    "    correct_labels = [b.correct for b in batch]\n",
    "    incorrect_labels = [b.incorrect for b in batch]\n",
    "\n",
    "    for latent_cache, correct, incorrect in zip(latents, correct_labels, incorrect_labels):\n",
    "        latent_cache.correct_label = correct\n",
    "        latent_cache.incorrect_label = incorrect\n",
    "        latent_cache.group=\"relations\"\n",
    "    \n",
    "    lcc = LatentCacheCollection(latents=latents)\n",
    "    lcc.detensorize()\n",
    "\n",
    "    with open(os.path.join(cache_dir, f\"batch_{batch_idx}.json\"), \"w\") as f:\n",
    "        f.write(lcc.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_6.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_122.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_69.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_79.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_83.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_50.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_118.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_193.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_137.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_41.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_81.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_185.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_42.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_34.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_73.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_12.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_176.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_197.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_17.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_124.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_28.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_153.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_96.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_26.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_148.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_102.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_155.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_44.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_66.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_62.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_106.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_53.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_143.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_191.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_126.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_142.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_111.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_63.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_114.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_179.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_0.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_101.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_105.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_131.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_11.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_85.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_32.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_166.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_186.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_205.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_59.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_19.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_154.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_21.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_208.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_78.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_1.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_25.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_207.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_94.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_158.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_129.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_99.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_67.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_27.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_184.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_39.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_159.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_75.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_189.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_121.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_30.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_24.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_15.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_165.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_48.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_37.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_60.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_33.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_161.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_36.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_13.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_125.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_40.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_115.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_108.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_117.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_157.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_173.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_72.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_31.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_29.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_49.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_128.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_180.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_164.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_199.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_89.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_74.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_116.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_202.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_8.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_209.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_107.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_201.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_71.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_135.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_132.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_169.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_120.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_76.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_109.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_150.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_90.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_10.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_58.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_194.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_95.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_82.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_9.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_175.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_5.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_43.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_123.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_38.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_144.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_16.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_187.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_203.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_138.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_45.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_149.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_92.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_133.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_55.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_160.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_104.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_147.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_181.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_151.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_2.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_70.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_23.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_156.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_56.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_100.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_182.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_210.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_112.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_61.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_86.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_190.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_198.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_87.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_141.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_178.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_35.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_47.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_136.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_146.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_140.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_206.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_18.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_97.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_167.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_80.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_57.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_162.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_54.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_88.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_172.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_93.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_91.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_98.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_195.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_68.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_110.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_145.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_52.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_77.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_14.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_188.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_152.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_65.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_127.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_20.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_113.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_119.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_204.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_139.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_192.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_22.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_46.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_171.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_130.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_177.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_4.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_7.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_84.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_51.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_103.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_174.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_3.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_196.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_170.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_134.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_183.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_64.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_200.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_168.json',\n",
       " '/home/local_arnab/Codes/Projects/talkative_probes/results/cached_latents/Llama-3.2-3B/tense/tense/batch_163.json']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.activation_manager import ActivationLoader, get_batch_paths\n",
    "\n",
    "latent_root = os.path.join(\n",
    "    env_utils.DEFAULT_RESULTS_DIR, \n",
    "    \"cached_latents\", MODEL_KEY.split(\"/\")[-1], \"tense\"\n",
    ")\n",
    "\n",
    "activation_batch_paths = list(get_batch_paths(latent_root))\n",
    "activation_batch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_loader = ActivationLoader(\n",
    "    latent_cache_files=activation_batch_paths,\n",
    "    shuffle=True,\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.train_utils import evaluate\n",
    "\n",
    "evaluate(mt=mt, eval_set = act_loader, limit_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"# Does this passage concern a male person?\",\n",
    "    \"# Does this text explore Business?\",\n",
    "    \"# Is the subject of this sentence female?\",\n",
    "    \"# Does this review indicate a poor experience?\",\n",
    "    \"# Is this talking about a female person?\",\n",
    "    \"# Does the reviewer appear displeased with their experience?\",\n",
    "    \"# Does this refer to a male individual?\",\n",
    "    \"# Is the overall message of this review negative?\",\n",
    "    \"# Can this article be categorized under World News?\",\n",
    "    \"# Can this article be categorized under Business?\",\n",
    "    \"# In terms of content, does this piece deal with World News?\",\n",
    "    \"# Is Central African Republic mentioned anywhere in the passage?\",\n",
    "    \"# Could you tell me if this is factual?\",\n",
    "    \"# Is Rally of in any part of this passage?\",\n",
    "    \"# Is Israeli cited in this piece?\",\n",
    "    \"# Is there a positive tone throughout this review?\",\n",
    "    \"# Does this describe someone who is female?\",\n",
    "    \"# Does this text explore Sports?\",\n",
    "    \"# Is Japanese referenced in this text?\",\n",
    "    \"# Could this be summarized as an article about Business?\",\n",
    "    \"# Can you find any references to since November in this text?\",\n",
    "    \"# Would you classify this as an endorsement?\",\n",
    "    \"# Are you confirming this is true?\",\n",
    "    \"# Is 1498 discussed anywhere in this content?\",\n",
    "    \"# Does this text refer to March?\",\n",
    "    \"# Is there a substantial focus on Business in this article?\",\n",
    "    \"# Is the general sentiment of this review negative?\",\n",
    "    # \"# Does Rwanda \\x92s appear in this text?\",\n",
    "    \"# Is the sentiment in this review on the negative side?\",\n",
    "    \"# Does this article delve into matters concerning Science/Technology?\",\n",
    "    \"# Is there a substantial focus on World News in this article?\",\n",
    "    \"# Is this sentence about a female person?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 12:35:23 numexpr.utils INFO     Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-10-30 12:35:23 numexpr.utils INFO     NumExpr defaulting to 8 threads.\n",
      "2024-10-30 12:35:23 datasets INFO     PyTorch version 2.5.0 available.\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018 HTTP/11\" 200 1480\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/FrancophonIA/WiLI-2018/FrancophonIA/WiLI-2018.py HTTP/11\" 404 0\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018 HTTP/11\" 200 1480\n",
      "2024-10-30 12:35:23 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/FrancophonIA/WiLI-2018/resolve/d505038b20006eb17994578c1e870d443f1cd5d2/README.md HTTP/11\" 200 0\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/FrancophonIA/WiLI-2018/resolve/d505038b20006eb17994578c1e870d443f1cd5d2/.huggingface.yaml HTTP/11\" 404 0\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://datasets-server.huggingface.co:443 \"GET /info?dataset=FrancophonIA/WiLI-2018 HTTP/11\" 200 None\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018/revision/d505038b20006eb17994578c1e870d443f1cd5d2 HTTP/11\" 200 1480\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018/tree/d505038b20006eb17994578c1e870d443f1cd5d2?recursive=False&expand=False HTTP/11\" 200 412\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018/tree/d505038b20006eb17994578c1e870d443f1cd5d2/data?recursive=False&expand=False HTTP/11\" 404 79\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"GET /api/datasets/FrancophonIA/WiLI-2018/revision/d505038b20006eb17994578c1e870d443f1cd5d2 HTTP/11\" 200 1480\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    Starting new HTTPS connection (1): huggingface.co:443\n",
      "2024-10-30 12:35:24 urllib3.connectionpool DEBUG    https://huggingface.co:443 \"HEAD /datasets/FrancophonIA/WiLI-2018/resolve/d505038b20006eb17994578c1e870d443f1cd5d2/dataset_infos.json HTTP/11\" 404 0\n",
      "2024-10-30 12:35:24 filelock DEBUG    Attempting to acquire lock 140562955438480 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_FrancophonIA___wi_li-2018_default_0.0.0_d505038b20006eb17994578c1e870d443f1cd5d2.lock\n",
      "2024-10-30 12:35:24 filelock DEBUG    Lock 140562955438480 acquired on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_FrancophonIA___wi_li-2018_default_0.0.0_d505038b20006eb17994578c1e870d443f1cd5d2.lock\n",
      "2024-10-30 12:35:24 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2/dataset_info.json\n",
      "2024-10-30 12:35:24 filelock DEBUG    Attempting to release lock 140562955438480 on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_FrancophonIA___wi_li-2018_default_0.0.0_d505038b20006eb17994578c1e870d443f1cd5d2.lock\n",
      "2024-10-30 12:35:24 filelock DEBUG    Lock 140562955438480 released on /home/local_arnab/.cache/huggingface/datasets/_home_local_arnab_.cache_huggingface_datasets_FrancophonIA___wi_li-2018_default_0.0.0_d505038b20006eb17994578c1e870d443f1cd5d2.lock\n",
      "2024-10-30 12:35:24 filelock DEBUG    Attempting to acquire lock 140562949983248 on /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2_builder.lock\n",
      "2024-10-30 12:35:24 filelock DEBUG    Lock 140562949983248 acquired on /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2_builder.lock\n",
      "2024-10-30 12:35:24 fsspec.local DEBUG    open file: /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2/dataset_info.json\n",
      "2024-10-30 12:35:24 filelock DEBUG    Attempting to release lock 140562949983248 on /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2_builder.lock\n",
      "2024-10-30 12:35:24 filelock DEBUG    Lock 140562949983248 released on /home/local_arnab/.cache/huggingface/datasets/FrancophonIA___wi_li-2018/default/0.0.0/d505038b20006eb17994578c1e870d443f1cd5d2_builder.lock\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ContextQASample(context='perhitungan dasar yang berlaku sejak zaman pertengahan adalah paskah dirayakan pada hari minggu setelah bulan purnama pertama setelah hari pertama musim semi vernal equinox kalimat tersebut sebenarnya tidak tepat benar dengan sistem perhitungan gerejawi', questions=['# Is this text predominantly in Korean?', '# Is this text predominantly in Indonesian?', '# Is this passage composed in Chinese?', '# Is this passage composed in Swedish?', '# Is this written in the language Romanian?', '# Would you classify this as written in Indonesian?', '# Is this text written in Indonesian?', '# Can we confirm this is in Urdu?', '# Is the text presented in Indonesian?', '# Is this text predominantly in Russian?'], answers=['No', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No']),\n",
       " ContextQASample(context='denna typ av kommersiell och välproducerad rock fortsatte på deras nästa skiva outside inside som kom  på denna skiva kom deras första top -hit på billboardlistan \"shes a beauty\" en låt där för övrigt totos dåvarande gitarrist steve lukather medverkade  kom love bomb producerad av todd rundgren som de tidigare arbetat med på remote control skivan blev en total flopp och gruppen splittrades kort därefter', questions=['# Am I correct in saying this is in Swedish?', '# Is this text written in Swedish?', '# Is the content of this text in Swedish?', '# Is this text predominantly in Swedish?', '# Is the text presented in Chinese?', '# Is this passage composed in Latin?', '# Is this written in the language Portugese?', '# Can we confirm this is in Swedish?', '# Does this writing reflect the language Turkish?', '# Can we determine that this is in Swedish?'], answers=['Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes']),\n",
       " ContextQASample(context='لا يعتمد اختيار أي نظام اتصال من بعد لتحقيق أغراض معينة على التقنية الممكن استخدامها وكلفتها فقط بل على الكلفة النسبية للحلول المختلفة المتوافرة ويلجأ للموازنة بين مختلف الحلول والمنظومات المحتملة اقتصادياً إلى دراسة القيمة الحالية للكلفة السنوية present value of annoal chargespvac التي تراعي كلاً من رأس المال والنفقات المستمرة للمشروع في المدة الزمنية الكلية المتوقعة لاستثماره والتي لا تقل عن عشرين سنة أما من وجهة نظر مميزات التمويل النقدي لمختلف النفقات فإن أفضل المشاريع هي التي يمكن تجزئتها إلى مراحل لتفي كل مرحلة منها بالمتطلبات الحالية والمتوقعة لمدة زمنية قصيرة', questions=['# Am I correct in saying this is in Turkish?', '# Is this written in the language Arabic?', '# Can we confirm this is in Arabic?', '# Is the language of this text Arabic?', '# Is the content of this text in Estonian?', '# Can we determine that this is in Arabic?', '# Is the language used here Russian?', '# Is this passage composed in Arabic?', '# Is the language used here Spanish?', '# Can we confirm this is in Arabic?'], answers=['No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes']),\n",
       " ContextQASample(context='comodo securebox - temmuz  yılında piyasaya sürülen ve ortaklaşa halihazırda tehlikelerin bu tür finansal işlemler gibi hassas faaliyetleri yürütmek için gereken bir uygulama olarak problem çözme niyetiyle western union ile geliştirilmiştir securebox çevreleme teknolojisi uygulamasının etkinlikleri bilgisayara kötücül yazılım bulaşmış olsa bile korur', questions=['# Can we say this is in Turkish?', '# Is the content of this text in Chinese?', '# Is the language of this text Turkish?', '# Would you agree that this is in Tamil?', '# Can we determine that this is in Turkish?', '# Does this passage use Russian?', '# Is this text written in Turkish?', '# Can we determine that this is in Turkish?', '# Is this text predominantly in Turkish?', '# Is this passage composed in Tamil?'], answers=['Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No']),\n",
       " ContextQASample(context='आइनस्टाइन ने कहा कि आपेक्षिकता के सिद्धान्त के अनुसार यह विचार कि भौतिक वस्तुएँ एक दूसर को आकर्षित करती हैं एक भ्रम है जो प्रकृति संबंधी गलत याँत्रिक धारणाओं के कारण पैदा हुआ है। उन्होंने कहा कि पृथ्वी बड़ी है और उसकी वजह से उसके इर्द-गिर्द का दिक्-काल मुड़ गया है और अपने ऊपर तह हो गया है। हम इस खिचे-मुड़े दिक्-काल में रहते हैं और इस मुड़न की वजह से पृथ्वी के क़रीब धकेले जाते हैं। इसकी तुलना एक चादर से की जा सकती है जिसके चार कोनो को चार लोगों ने खींच के पकड़ा हो। अब इस चादर के बीच में एक भारी गोला रख दिया जाए तो चादर बीच से बैठ जाएगी यानि उसके सूत में बीच में मुड़न पैदा हो जाएगी। अब अगर एक हलकी गेंद हम चादर के कोने पर रखे तो वह लुड़क कर बड़े गोले की तरफ़ जाएगी। आइनस्टाइन ने कहा कि कोई अनाड़ी आदमी यह देख कर कह सकता है कि छोटी गेंद को बड़े गोले ने खींचा इसलिए गेंद उसके पास गई। लेकिन असली वजह थी कि गेंद ज़मीन की तरफ़ जाना चाहती थी और गोले ने चादर में कुछ ऐसी मुड़न पैदा की कि गेंद उसके पास चली गई। इसी तरह से उन्होंने कहा कि यह एक मिथ्या है कि गुरुत्वाकर्षण किसी आकर्षण की वजह से होता है। गुरुत्वाकर्षण की असली वजह है कि हर वस्तु जो अंतरिक्ष में चल रही होती है वह दिक् के ऐसी मुड़न के प्रभाव में आकर किसी बड़ी चीज़ की ओर चलने लगती है।', questions=['# Can we say this is in Hindi?', '# Does this passage use Pushto?', '# Is the content of this text in Russian?', '# Is this written in the language Estonian?', '# Is this written in the language Chinese?', '# Is this text predominantly in Hindi?', '# Is the language used here Hindi?', '# Can we say this is in Spanish?', '# Am I correct in saying this is in Hindi?', '# Would you identify this text as being in Hindi?'], answers=['Yes', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataset_manager import DatasetManager\n",
    "dataloader = DatasetManager.from_named_datasets(\n",
    "    # [(\"geometry_of_truth\", \"cities\")],\n",
    "    # [(\"sst2\", \"sst2\")],\n",
    "    # [(\"relations\", 'factual/country_capital_city')],\n",
    "    # [(\"tense\", \"tense\")],\n",
    "    [(\"language_identification\", \"language_identification\")],\n",
    "    batch_size=5\n",
    ")\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perhitungan dasar yang berlaku sejak zaman pertengahan adalah paskah dirayakan pada hari minggu setelah bulan purnama pertama setelah hari pertama musim semi vernal equinox kalimat tersebut sebenarnya tidak tepat benar dengan sistem perhitungan gerejawi',\n",
       " 'denna typ av kommersiell och välproducerad rock fortsatte på deras nästa skiva outside inside som kom  på denna skiva kom deras första top -hit på billboardlistan \"shes a beauty\" en låt där för övrigt totos dåvarande gitarrist steve lukather medverkade  kom love bomb producerad av todd rundgren som de tidigare arbetat med på remote control skivan blev en total flopp och gruppen splittrades kort därefter',\n",
       " 'لا يعتمد اختيار أي نظام اتصال من بعد لتحقيق أغراض معينة على التقنية الممكن استخدامها وكلفتها فقط بل على الكلفة النسبية للحلول المختلفة المتوافرة ويلجأ للموازنة بين مختلف الحلول والمنظومات المحتملة اقتصادياً إلى دراسة القيمة الحالية للكلفة السنوية present value of annoal chargespvac التي تراعي كلاً من رأس المال والنفقات المستمرة للمشروع في المدة الزمنية الكلية المتوقعة لاستثماره والتي لا تقل عن عشرين سنة أما من وجهة نظر مميزات التمويل النقدي لمختلف النفقات فإن أفضل المشاريع هي التي يمكن تجزئتها إلى مراحل لتفي كل مرحلة منها بالمتطلبات الحالية والمتوقعة لمدة زمنية قصيرة',\n",
       " 'comodo securebox - temmuz  yılında piyasaya sürülen ve ortaklaşa halihazırda tehlikelerin bu tür finansal işlemler gibi hassas faaliyetleri yürütmek için gereken bir uygulama olarak problem çözme niyetiyle western union ile geliştirilmiştir securebox çevreleme teknolojisi uygulamasının etkinlikleri bilgisayara kötücül yazılım bulaşmış olsa bile korur',\n",
       " 'आइनस्टाइन ने कहा कि आपेक्षिकता के सिद्धान्त के अनुसार यह विचार कि भौतिक वस्तुएँ एक दूसर को आकर्षित करती हैं एक भ्रम है जो प्रकृति संबंधी गलत याँत्रिक धारणाओं के कारण पैदा हुआ है। उन्होंने कहा कि पृथ्वी बड़ी है और उसकी वजह से उसके इर्द-गिर्द का दिक्-काल मुड़ गया है और अपने ऊपर तह हो गया है। हम इस खिचे-मुड़े दिक्-काल में रहते हैं और इस मुड़न की वजह से पृथ्वी के क़रीब धकेले जाते हैं। इसकी तुलना एक चादर से की जा सकती है जिसके चार कोनो को चार लोगों ने खींच के पकड़ा हो। अब इस चादर के बीच में एक भारी गोला रख दिया जाए तो चादर बीच से बैठ जाएगी यानि उसके सूत में बीच में मुड़न पैदा हो जाएगी। अब अगर एक हलकी गेंद हम चादर के कोने पर रखे तो वह लुड़क कर बड़े गोले की तरफ़ जाएगी। आइनस्टाइन ने कहा कि कोई अनाड़ी आदमी यह देख कर कह सकता है कि छोटी गेंद को बड़े गोले ने खींचा इसलिए गेंद उसके पास गई। लेकिन असली वजह थी कि गेंद ज़मीन की तरफ़ जाना चाहती थी और गोले ने चादर में कुछ ऐसी मुड़न पैदा की कि गेंद उसके पास चली गई। इसी तरह से उन्होंने कहा कि यह एक मिथ्या है कि गुरुत्वाकर्षण किसी आकर्षण की वजह से होता है। गुरुत्वाकर्षण की असली वजह है कि हर वस्तु जो अंतरिक्ष में चल रही होती है वह दिक् के ऐसी मुड़न के प्रभाव में आकर किसी बड़ी चीज़ की ओर चलने लगती है।']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    b.context for b in batch\n",
    "]\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    791,   3363,    315,  12366,    374,    304,    279,   3224,\n",
       "            315]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'offset_mapping': tensor([[[ 0,  0],\n",
       "         [ 0,  3],\n",
       "         [ 3,  8],\n",
       "         [ 8, 11],\n",
       "         [11, 17],\n",
       "         [17, 20],\n",
       "         [20, 23],\n",
       "         [23, 27],\n",
       "         [27, 35],\n",
       "         [35, 38]]], device='cuda:0')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tokens import prepare_input, find_token_range\n",
    "\n",
    "prompts = [\n",
    "    # \"The land of\",\n",
    "    # \"The capital of France is\",\n",
    "    # \"This is a\"\n",
    "    \"The city of Paris is in the country of\"\n",
    "]\n",
    "\n",
    "batch_inputs = prepare_input(\n",
    "    tokenizer=mt,\n",
    "    prompts=prompts,\n",
    "    padding_side=\"left\",\n",
    "    # padding=\"max_length\",\n",
    "    # max_length=20,\n",
    "    truncation=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,    791,   3363,    315,  12366,    374,    304,    279,   3224,\n",
       "            315]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'offset_mapping': tensor([[[ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 3,  3],\n",
       "         [ 8,  8],\n",
       "         [11, 11],\n",
       "         [17, 17],\n",
       "         [20, 20],\n",
       "         [23, 23],\n",
       "         [27, 27],\n",
       "         [35, 35]]], device='cuda:0')}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.tokens import prepare_input, find_token_range\n",
    "\n",
    "batch_inputs = prepare_input(\n",
    "    tokenizer=mt,\n",
    "    prompts=prompts,\n",
    "    padding_side=\"left\",\n",
    "    # padding=\"max_length\",\n",
    "    # max_length=20,\n",
    "    truncation=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "batch_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|end_of_text|><|end_of_text|><|begin_of_text|>The land of',\n",
       " '<|begin_of_text|>The capital of France is',\n",
       " '<|end_of_text|><|end_of_text|><|begin_of_text|>This is a']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    mt.tokenizer.decode(inp, skip_special_tokens=False)\n",
    "    for inp in batch_inputs[\"input_ids\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_token_range(\n",
    "    string=prompts[1],\n",
    "    substring=\"is\",\n",
    "    tokenizer=mt,\n",
    "    offset_mapping=batch_inputs[\"offset_mapping\"][1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geometry_of_truth',\n",
       " 'relations',\n",
       " 'sst2',\n",
       " 'md_gender',\n",
       " 'snli',\n",
       " 'ag_news',\n",
       " 'ner',\n",
       " 'tense',\n",
       " 'language_identification',\n",
       " 'singular_plural']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(DatasetManager.list_datasets_by_group().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "200 * 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
