{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../chess_llm_interpretability\")\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "from src.utils import logging_utils\n",
    "from src.utils import env_utils\n",
    "from src import functional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=logging_utils.DEFAULT_FORMAT,\n",
    "    datefmt=logging_utils.DEFAULT_DATEFMT,\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "\n",
    "logger.info(f\"{torch.__version__=}, {torch.version.cuda=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import ModelandTokenizer\n",
    "\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.2-3B\"\n",
    "# MODEL_KEY = \"google/gemma-2-2b\"\n",
    "# MODEL_KEY = \"meta-llama/Llama-3.1-8B\"\n",
    "MODEL_KEY = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "mt = ModelandTokenizer(\n",
    "    model_key=MODEL_KEY,\n",
    "    # torch_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "finetuned_path = os.path.join(env_utils.DEFAULT_RESULTS_DIR, \"chess_model_cache\")\n",
    "finetuned_path = os.path.join(finetuned_path, os.listdir(finetuned_path)[-1])\n",
    "model = AutoModelForCausalLM.from_pretrained(finetuned_path, torch_dtype=torch.float32).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "layer_name = mt.mlp_module_name_format.format(5)\n",
    "\n",
    "w_o = baukit.get_module(mt._model, layer_name).down_proj.weight.data.to(torch.float32)\n",
    "w_f = baukit.get_module(model, layer_name).down_proj.weight.data.to(torch.float32)\n",
    "\n",
    "torch.allclose(w_o, w_f, atol = 1e-4)  # Check if weights are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt._model = model  # Replace the model in the ModelandTokenizer with the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a ChessBot, who can play chess with users. The user always plays as white and you play as black. The user will give their move in PGN format. You should respond with your move in PGN format. If you cannot find a move, you should resign.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chess_llm_interpretability import chess_utils\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def format_move(move):\n",
    "    if \".\" in move:\n",
    "        move = move.split(\".\")[-1]\n",
    "    if move[-1] == \"#\":\n",
    "        move = move[:-1]\n",
    "    return move.strip()\n",
    "\n",
    "def format_pgn_string(pgn_string):\n",
    "    white_moves = []\n",
    "    black_moves = []\n",
    "    for idx, move in enumerate(pgn_string.split(\" \")):    \n",
    "        if idx % 2 == 0:\n",
    "            white_moves.append(format_move(move))\n",
    "        else:\n",
    "            black_moves.append(format_move(move))\n",
    "    return white_moves, black_moves\n",
    "\n",
    "def get_white_and_black_moves(pgn_string):\n",
    "    white_move_indices = chess_utils.get_all_white_pos_indices(pgn_string)\n",
    "    white_moves = [\n",
    "        format_move(\"\".join([pgn_string[jdx] for jdx in white_move_indices[idx]]))\n",
    "        for idx in range(len(white_move_indices))\n",
    "    ]\n",
    "\n",
    "    black_move_indices = chess_utils.get_all_black_pos_indices(pgn_string)\n",
    "    black_moves = [\n",
    "        format_move(\"\".join([pgn_string[jdx] for jdx in black_move_indices[idx]]))\n",
    "        for idx in range(len(black_move_indices))\n",
    "    ]\n",
    "\n",
    "    return white_moves, black_moves\n",
    "\n",
    "def get_prompt(\n",
    "    pgn_string: str,\n",
    "    query_move: int,\n",
    "    white_moves: Optional[list[str]] = None,\n",
    "    black_moves: Optional[list[str]] = None,\n",
    "    is_chat_mode: bool = False,\n",
    "):\n",
    "    if white_moves is None or black_moves is None:\n",
    "        white_moves, black_moves = get_white_and_black_moves(pgn_string)\n",
    "    \n",
    "    white_move_indices = chess_utils.get_all_white_pos_indices(pgn_string)\n",
    "    pgn_state = pgn_string[:white_move_indices[query_move-1][-1] + 1]\n",
    "    if is_chat_mode == False:\n",
    "        return dict(\n",
    "            prompt=f\"PGN of a chess game: {pgn_state}\",\n",
    "            answer=black_moves[query_move-1],\n",
    "            pgn_state=pgn_state,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            # {\"role\": \"user\", \"content\": \"d4\"},\n",
    "            # {\"role\": \"assistant\", \"content\": \"c5\"},\n",
    "            # {\"role\": \"user\", \"content\": \"c3\"},\n",
    "        ]\n",
    "        for idx, (white_move, black_move) in enumerate(zip(white_moves[:query_move], black_moves[:query_move])):\n",
    "            conversation.append({\"role\": \"user\", \"content\": white_move})\n",
    "            if idx != query_move-1:\n",
    "                conversation.append({\"role\": \"assistant\", \"content\": black_move})\n",
    "        formatted_chat = mt.tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "        return dict(\n",
    "            prompt = formatted_chat, \n",
    "            answer = black_moves[query_move-1],\n",
    "            pgn_state = pgn_state,\n",
    "        )\n",
    "\n",
    "pgn_string = \"1.e4 e5 2.Bc4 Bc5 3.c3 Nf6 4.d3 d6 5.h3 Be6 6.Bb5+ c6 7.Ba4 Qd7 8.b4 Bb6 9.Na3 h6 10.b5 c5 11.g4 Qe7 12.g5 Ng8 13.h4 hxg5 14.Bxg5 f6 15.Be3 Nd7 16.d4 exd4 17.cxd4 cxd4 18.Bxd4 Ba5+ 19.Kf1 b6 20.Rc1 Bb4 21.Nc2 Bc4+ 22.Ne2 Qxe4 23.Rh3 Bxe2+ 24.Qxe2 Qxe2+ 25.Kxe2 Bc5 26.Bb3 Ne7 27.Bxc5 dxc5 28.Rd1 Ne5 29.Re1 N7g6 30.Kf1 O-O-O 31.Be6+ Kb8 32.Bf5 Nxh4 33.Ne3 g6 34.Be6 Rd6 35.Bb3 g5 36.Nd5 c4 37.Bxc4 Nxc4 38.Nxb6 axb6 39.Rc1 Nd2+ 40.Ke2 Re8+ 41.Kd1 Nb3+ 42.Kc2 Nxc1 43.Kxc1\"\n",
    "query_move = 5\n",
    "\n",
    "request = get_prompt(\n",
    "    pgn_string, query_move,\n",
    "    is_chat_mode=False\n",
    ")\n",
    "\n",
    "print(request[\"prompt\"])\n",
    "print(request[\"answer\"])\n",
    "print(request[\"pgn_state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = chess_utils.pgn_string_to_board(request[\"pgn_state\"])\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_black_moves = [board.san(move) for move in board.legal_moves]\n",
    "# legal_black_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def get_next_move_by_LM(\n",
    "    request: dict,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    prompt = request[\"prompt\"]\n",
    "    answer = request[\"answer\"]\n",
    "    pgn_state = request[\"pgn_state\"]\n",
    "\n",
    "    inputs = mt.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    inputs = {key: tensor.to(mt.device) for key, tensor in inputs.items()}\n",
    "\n",
    "    outputs = mt._model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=12,\n",
    "        do_sample=False, \n",
    "        # temperature=0.01\n",
    "    )\n",
    "    # logger.debug(f\"Generated tokens:\\n{outputs}\")\n",
    "\n",
    "    decoded_output = mt.tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "\n",
    "    if debug:\n",
    "        logger.debug(f\"{pgn_state=} | {answer=}\")\n",
    "        logger.debug(f\"{prompt=}\")\n",
    "        logger.debug(\"-\"*50)\n",
    "        logger.debug(f\"Decoded output:\\n{decoded_output}\")\n",
    "\n",
    "    return decoded_output.strip().split(\" \")[0].strip()\n",
    "\n",
    "next_black_move = get_next_move_by_LM(request, debug = True)\n",
    "logger.info(f\"{next_black_move} | {next_black_move in legal_black_moves=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy of Legal Moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"adamkarvonen/chess_games\", data_files=\"lichess_100mb.zip\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 100\n",
    "pgn_transcripts = []\n",
    "for d in dataset[\"train\"]:\n",
    "    pgn_transcripts.append(d[\"transcript\"])\n",
    "    if len(pgn_transcripts) >= limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_wrong_ans = []\n",
    "LIMIT_MOVES = 30\n",
    "track_accuracy = {move_idx: {\"n_correct\": 0, \"n_total\": 0} for move_idx in range(1, LIMIT_MOVES)}\n",
    "\n",
    "from tqdm import tqdm\n",
    "for pgn_string in tqdm(pgn_transcripts):\n",
    "    white_moves, black_moves = get_white_and_black_moves(pgn_string)\n",
    "    for query_move in range(\n",
    "        1, min(\n",
    "            len(white_moves), \n",
    "            len(black_moves), \n",
    "            LIMIT_MOVES - 1\n",
    "        )\n",
    "    ):\n",
    "        track_accuracy[query_move][\"n_total\"] += 1\n",
    "\n",
    "        request = get_prompt(\n",
    "            pgn_string, query_move,\n",
    "            white_moves=white_moves,\n",
    "            black_moves=black_moves,\n",
    "            # is_chat_mode=(\"instruct\" in MODEL_KEY.lower()) or (\"chat\" in MODEL_KEY.lower())\n",
    "            is_chat_mode=False\n",
    "        )\n",
    "\n",
    "        next_black_move = get_next_move_by_LM(\n",
    "            request, debug=False\n",
    "        )\n",
    "\n",
    "\n",
    "        board = chess_utils.pgn_string_to_board(request[\"pgn_state\"])\n",
    "        legal_black_moves = [board.san(move) for move in board.legal_moves]\n",
    "\n",
    "        track_accuracy[query_move][\"n_correct\"] += next_black_move in legal_black_moves\n",
    "\n",
    "        if next_black_move not in legal_black_moves:\n",
    "            track_wrong_ans.append(dict(\n",
    "                pgn_string=pgn_string,\n",
    "                query_move=query_move,\n",
    "                next_black_move=next_black_move,\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_legal = []\n",
    "for query_move in range(1, LIMIT_MOVES):\n",
    "    if track_accuracy[query_move][\"n_total\"] == 0:\n",
    "        break\n",
    "    accuracy_legal.append(\n",
    "        track_accuracy[query_move][\"n_correct\"] / track_accuracy[query_move][\"n_total\"]\n",
    "    )\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.bar(x = range(1, len(accuracy_legal)+1), height = accuracy_legal)\n",
    "plt.xticks(range(1, len(accuracy_legal)+1), rotation=45)\n",
    "plt.xlabel(\"Move number (Black)\")\n",
    "plt.ylabel(f\"Accuracy (in legal moves, out of {limit})\")\n",
    "# plt.title(f\"{MODEL_KEY} - Finetuned on {7000*6} PGN Games\")\n",
    "plt.title(f\"{MODEL_KEY}\")\n",
    "\n",
    "plt.savefig(\"accuracy_plot.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_wrong_ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = get_prompt(\n",
    "    pgn_string=track_wrong_ans[0][\"pgn_string\"],\n",
    "    query_move=track_wrong_ans[0][\"query_move\"],\n",
    "    is_chat_mode=False\n",
    ")\n",
    "print(request[\"prompt\"])\n",
    "print(request[\"answer\"])\n",
    "print(request[\"pgn_state\"])\n",
    "\n",
    "board = chess_utils.pgn_string_to_board(request[\"pgn_state\"])\n",
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_black_moves = [board.san(move) for move in board.legal_moves]\n",
    "legal_black_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_black_move = get_next_move_by_LM(\n",
    "    request, debug=True\n",
    ")\n",
    "next_black_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_black_move in legal_black_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request[\"answer\"] in legal_black_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A quick brown fox jumps over the lazy\" + mt.tokenizer.eos_token\n",
    "\n",
    "inputs = mt.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(mt.device)\n",
    "outputs = mt._model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=12,\n",
    "    do_sample=False, \n",
    "    # temperature=0.01\n",
    ")\n",
    "# logger.debug(f\"Generated tokens:\\n{outputs}\")\n",
    "\n",
    "decoded_output = mt.tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = mt.tokenizer(\n",
    "    prompt, return_tensors=\"pt\", add_special_tokens=True,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=15,\n",
    ")\n",
    "\n",
    "[mt.tokenizer.decode(t) for t in inputs.input_ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
